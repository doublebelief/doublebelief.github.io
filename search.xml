<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[NAS论文阅读笔记]]></title>
    <url>%2F2019%2F08%2F19%2FNASpaperNotes%2F</url>
    <content type="text"><![CDATA[Neural Architecture Search with Reinforcement Learning Zoph, B., &amp; Le, Q. V. (2016). Neural Architecture Search with Reinforcement Learning. 1–16. Retrieved from http://arxiv.org/abs/1611.01578 Introduction Although Neural Architecture Application has become easier, designing architectures still requires a lot of expert knowledge and takes ample time. This paper presents Neural Architecture Search, a gradient-based method for finding good architectures (see Figure 1) . It is based on the observation that the structure and connectivity of a neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network – the controller – to generate such string. Training the network specified by the string – the “child network” – on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time. 神经网络架构的应用虽然比较广泛，但是设计神经网络架构依旧需要花费很多资源和时间。鉴于此，这篇论文提出了一个基于梯度算法的方法来搜索出最优的神经网络架构，这种方法也基于一个事实——神经网络的结构和连接通常能够被一个变长的字符串指定。因此，这篇论文提出了一个循环网络，这个循环网络包含一个控制器和一个“子网络”，控制器用于产生这样的变长字符串，之后根据这个字符串训练出相应的“子网络”, 并在验证集上进行验证，产生一个准确度，然后把这个准确度作为一个反馈信号，计算出策略梯度来更新控制器，因此，在下一次迭代中，控制器将会有更高概率产生高准确率的架构，换句话说，控制器将慢慢地优化对神经网络的搜索。 Related Work Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice. Despite their success, these methods are still limited in that they only search models from a fixed-length space. There are Bayesian optimization methods that allow to search non fixed length architectures (Bergstra et al., 2013; Mendoza et al., 2016), but they are less general and less flexible than the method proposed in this paper.Modern neuro-evolution algorithms，are much more flexible for composing novel models, yet they are usually less practical at a large scale. The controller in Neural Architecture Search is auto-regressive, which means it predicts hyperparameters one a time, conditioned on previous predictions. This idea is borrowed from the decoder in end-to-end sequence to sequence learning (Sutskever et al., 2014). Unlike sequence to sequence learning, our method optimizes a non-differentiable metric, which is the accuracy of the child network. It is therefore similar to the work on BLEU optimization in Neural Machine Translation (Ran- zato et al., 2015; Shen et al., 2016). Unlike these approaches, our method learns directly from the reward signal without any supervised bootstrapping. 超参数调优是在机器学习里是一个重要的研究课题，有好多方法都能得到好的效果，但是他们均只能在固定长度神经网络的搜索空间里搜索模型，另外，贝叶斯优化的方法虽然能够让这些方法搜索不定长的架构，但是这样的方法没有这篇论文中的方法更灵活。 一些现代的神经网络进化算法，在生成模型时更为灵活，但是它们通常不能用于大规模的计算。 在本文神经网络搜索里的控制器是自动回归的，也就是说它能基于之前的预测更新超参数。这个思想类似于端到端的序列学习里的解码器，但本文与之不同的是，本文的方法优化了一个不可微分的度量，这个度量是子网络的准确度。而这个度量的思想又类似于BLEU在神经网络机器翻译上的优化，但与这些方法不同的是，本文的方法直接按照反馈信息优化，而不需要监督指令。 Methods Generate Model Descriptions with A Controller Recurrent Neural Network(通过控制器循环网络产生模型) The process of generating an architecture stops if the number of layers exceeds a certain value. This value follows a schedule where we increase it as training progresses. Once the controller RNN finishes generating an architecture, a neural network with this architecture is built and trained. At convergence, the accuracy of the network on a held-out validation set is recorded. The parameters of the controller RNN, θc, are then optimized in order to maximize the expected validation accuracy of the proposed architectures. 如果网络层的数量超过了一个确定的值，那么产生神经网络架构的过程将会停止，其中这个确定的值是随着训练进程不断增加的。一旦循环网络控制器停止产生神经网络架构，那么，基于这个架构的神经网络则会被构建和训练，当神经网络趋于收敛时，这个神经网络在被留出的验证集上的准确度将会被记录。接着循环网络控制器的参数θc 将会被优化，以便产生最大期望验证集准确率的神经架构。 Training with Reinforce(利用强化学习进行训练) Use this accuracy R as the reward signal and use reinforcement learning to train the controller. More concretely, to find the optimal architecture, we ask our controller to maximize its expected reward. Since the reward signal R is non-differentiable, we need to use a policy gradient method to iteratively update θc. As training a child network can take hours, we use distributed training and asynchronous parameter updates in order to speed up the learning process of the controller . We use a parameter-server scheme where we have a parameter server of S shards, that store the shared parameters for K controller replicas. Each controller replica samples m different child architectures that are trained in parallel. The controller then collects gradients according to the results of that minibatch of m architectures at convergence and sends them to the parameter server in order to update the weights across all controller replicas. In our implementation, convergence of each child network is reached when its training exceeds a certain number of epochs. This scheme of parallelism is summarized in Figure 3 这篇论文使用模型准确度R作为反馈信号，并使用强化学习来训练控制器，具体来讲，为了找到最优的架构，控制器需要产生一个有最大准确度的模型架构。但是因为反馈信号R是不可微分的，所以这篇文章使用策略梯度方法来迭代更新控制器参数θc。 因为训练一个子网络会耗费大量时间，这篇论文采用了分布式训练和异步参数更新的方法来加快控制器的训练进程。这个方法具体来讲就是，使用一组s个参数服务器分片，这组参数服务器包含着k个控制器副本的参数，并且s个参数服务器向这k个控制器副本提供参数。而每个控制器副本分成m个不同的子架构，并且这每个不同的子架构平行训练，训练后的准确度将会被记录，然后计算关于控制器参数θc的梯度，而这个梯度将会被返回到参数服务器，进行迭代更新。 Increase Architecture Complexity with Skip Connections and Other Layer Types(通过跳过连接来增加结构复杂度以及其他网络层类型) We introduce a method that allows our controller to propose skip connections or branching layers, thereby widening the search space. To enable the controller to predict such connections, we use a set-selection type attention which was built upon the attention mechanism. At layer N, we add an anchor point which has N− 1 content-based sigmoids to indicate the previous layers that need to be connected. Each sigmoid is a function of the current hidden state of the controller and the previous hidden states of the previous N − 1 anchor points. We then sample from these sigmoids to decide what previous layers to be used as inputs to the current layer. To be able to add more types of layers, we need to add an additional step in the controller RNN to predict the layer type, then other hyperparameters associated with it. 这篇论文介绍了一种方法，这种方法能够让控制器跳过连接或分支层，进而可以拓宽搜索空间。这种方法引入了注意力机制，在控制器网络第N层增加了一个定位点，这个定位点有N-1个表示前面需要连接的网络层的激活函数。每一个激活函数都是一个关于当前层隐藏状态和前n-1个定位点隐藏状态的函数。这种方法就会根据这个函数来确定前面的网络层是否作为当前层的输入。 为了能够增加更多类型的网络层（不仅仅是卷积层，还有池化层，标准化层等）,需要在控制器的循环神经网络中增加一个额外的步骤来预测网络层的类型和与之关联一些的超参数。 Generate Recurrent Cell Architectures(产生循环网络架构) The computations for basic RNN and LSTM cells can be generalized as a tree of steps that take xt and ht−1 as inputs and produce ht as final output. The controller RNN needs to label each node in the tree with a combination method (addition, elementwise multiplication, etc.) and an activation function (tanh, sigmoid, etc.) to merge two inputs and produce one output. Two outputs are then fed as inputs to the next node in the tree. To allow the controller RNN to select these methods and functions, we index the nodes in the tree in an order so that the controller RNN can visit each node one by one and label the needed hyperparameters. 基本的RNN和LSTM结构的计算，能够被一般化为一棵阶梯树，它以xt和h(t-1)作为输入，产生h(t)作为最后输出。这个控制器循环网络需要标注在阶梯树里的每个节点，并且使用一个结合方法和一个激活函数来合并两个输入并产生一个输出，两个输出之后又被作为树中下一个节点的输入。为了允许控制器循环网络能够选择这些方法和函数，这里为阶梯树上的每一个节点按照次序创建了索引，以便控制器循环网络能够依次遍历每一个节点，并且标注需要的超参数。 Experiments and Results Learning Convolutional Architectures For CIFAR-10(在CIFAR-10数据集上学习卷积神经网络) Training details: The controller RNN is a two-layer LSTM with 35 hidden units on each layer. It is trained with the ADAM optimizer (Kingma &amp; Ba, 2015) with a learning rate of 0.0006. The weights of the controller are initialized uniformly between -0.08 and 0.08. For the distributed train- ing, we set the number of parameter server shards S to 20, the number of controller replicas K to 100 and the number of child replicas m to 8, which means there are 800 networks being trained on 800 GPUs concurrently at any time.Once the controller RNN samples an architecture, a child model is constructed and trained for 50 epochs. The reward used for updating the controller is the maximum validation accuracy of the last 5 epochs cubed. The validation set has 5,000 examples randomly sampled from the training set, the remaining 45,000 examples are used for training. The settings for training the CIFAR-10 child models are the same with those used in Huang et al. (2016a). We use the Momentum Optimizer with a learning rate of 0.1, weight decay of 1e-4, momentum of 0.9 and used Nesterov Momentum (Sutskever et al., 2013).During the training of the controller, we use a schedule of increasing number of layers in the child networks as training progresses. On CIFAR-10, we ask the controller to increase the depth by 2 for the child models every 1,600 samples, starting at 6 layers. Results: 训练细节：控制器循环网络采用两层的LSTM，每层有35个隐藏单元，并采用学习率是0.0006的ADAM优化器。控制器的初始换权重是在-0.08到0.08之间。对于分布式训练来讲，参数服务器分片S是20个，控制器副本K是100个，子网络架构m是8个，所以总共可以有800个网络同时在800个GPU上训练。每一个子网络架构训练50各阶段，更新控制器使用的反馈参数是最后五个阶段中最大的验证准确率。验证集是从训练数据中随机选择的5000个样本，而剩下的45000个样本则作为训练集。在训练控制器的过程中，使用了一个随时间增长的子网络层数，初始层数是6层，每训练1600个样本就增加2层。 训练结果：神经网络架构搜索能够设计几个表现优良的网络架构，其在CIFAR-10上的表现性能和一些最好的模型差不多。 Learning Recurrent Cells for Penn TreeBank(在PTB数据集上学习循环神经网络) Training details: The controller and its training are almost identical to the CIFAR-10 experiments except for a few modifications: 1) the learning rate for the controller RNN is 0.0005, slightly smaller than that of the controller RNN in CIFAR-10, 2) in the distributed training, we set S to 20, K to 400 and m to 1, which means there are 400 networks being trained on 400 CPUs concurrently at any time, 3) during asynchronous training we only do parameter updates to the parameter-server once 10 gradients from replicas have been accumulated. Results: 训练细节：这个实验的控制器循环网络和训练与上面（CIFAR-10）的实验类似，主要有以下几个不同： 优化器的学习率是0.005; 分布式训练中，参数服务器S是20个，控制器副本K是400个，子网络架构m是1个； 在异步训练中，只有控制器副本积累到10个梯度时才更新参数服务器的参数。 训练结果：神经网络搜索获取到的模型在该数据集上表现优于其他一些先进的模型。另外，还可以将获取到的模型通过迁移学习使用在其他的问题解决上。 Conclusion Neural Architecture Search is an idea of using a recurrent neural network to compose neural network architectures. By using recurrent network as the controller, our method is flexible so that it can search variable-length architecture space. Our method has strong empirical performance on very challenging benchmarks and presents a new research direction for automatically finding good neural network architectures. NAS是一种使用循环神经网络生成神经网络架构的想法，通过使用循环神经网络作为控制器，这种方法能够很灵活的用于搜索变长的架构空间，而且在一些数据集上具有很好的实验性能，为自动化查找神经网络架构提供了一个新的研究方向。 Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation Liu, C., Chen, L.-C., Schroff, F., Adam, H., Hua, W., Yuille, A., &amp; Fei-Fei, L. (2019). Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation. Retrieved from http://arxiv.org/abs/1901.02985 Introduction In this paper, we study Neural Archi- tecture Search for semantic image segmentation, an important computer vision task that assigns a label like “person” or “bicycle” to each pixel in the input image. The vast majority of current works on NAS follow this two-level hierarchical design, but only automat- ically search the inner cell level while hand-designing the outer network level. We propose a network level architecture search space that augments and complements the much-studied cell level one, and consider the more challenging joint search of network level and cell level architectures. We develop a differentiable, continuous formulation that conducts the two-level hierarchical architecture search efficiently in 3 GPU days. On PASCAL VOC 2012 and ADE20K, our best model outper- forms several state-of-the-art models. 这篇论文将神经网络架构搜索运用在图像语义分割上。图像语义分割是计算即视觉方面的一个重要任务，目的是给图像中的每一个像素点都打上标签。 当前很多的NAS主要遵循一个两级的分层设计，但是只自动化搜索内部的核单元设计，至于外部的网络设计则采用人工设计。这篇论文提出了一个外部网络架构搜索空间，增加了研究较多的内部核单元搜索，并且进一步考虑了联合外部网络的搜索和内部核单元的搜索。 这篇论文也提出了一个可微分的连续方程，能够使两级分层架构能够在使用GPU的情况下3天完成任务。此外，根据这种方法得到模型在一些数据集上表现性能良好。 Related Work Convolutional neural networks deployed in a fully convolutional manner (FCNs) have achieved remarkable performance on several semantic segmentation benchmarks. Within the state-of-the-art systems, there are two essential components: multi-scale context module and neural network design. In this work, we apply neural architecture search for network backbones specific for semantic segmentation. We further show state-of-the-art performance without ImageNet pretraining, and significantly outperforms FRRN and GridNet on Cityscapes. Several papers used reinforcement learning (either policy gradients or Q-learning ) to train a recurrent neural network that represents a policy to generate a sequence of symbols specifying the CNN architecture. An alternative to RL is to use evolutionary algorithms (EA), that “evolves” architectures by mutating the best architectures found so far. These RL and EA methods tend to require massive computation during the search, usually thousands of GPU days. Our work follows the differentiable NAS formulation and extends it into the more general hierarchical setting. Our work still uses this cell level search space to keep consistent with previous works. Yet one of our contributions is to propose a new, general-purpose network level search space, since we wish to jointly search across this two-level hierarchy. 把卷积神经网络运用到一个全卷积网络里，可以很好的解决语义分割的问题，但是要真正实现这个全卷积网络需要精心的设计和多范围数据的模块。这篇论文里将NAS用于解决这个问题，可以在不需要预训练的情况下获得一个很好的模型。 一些论文中使用强化学习来训练循环神经网络进而产生CNN架构的符号序列，一个强化学习的变体，进化算法也能通过”变异“来产生最终架构。这些方法在查找过程中都需要耗费大量的计算资源，而这篇论文则使用了可微分的NAS公式，并且将它推广到更通用的分层设置中，使得不必要挨个的训练模型，可以节省很多开销。 此外，这篇论文使用了之前工作中的核单位搜索空间，并加入了自己的贡献，即提出了一个新的，外在网络搜索空间，并且尽量通过两级分层结构将二者联合搜索。 Architecture Search Space For the inner cell level , we reuse the one adopted in to keep consistent with previous works. For the outer network level , we propose a novel search space based on observation and summarization of many popular designs. We define a cell to be a small fully convolutional module,typically repeated multiple times to form the entire neural network. More specifically, a cell is a directed acyclic graph consisting of B blocks. We propose the following network level search space. The beginning of the network is a two-layer “stem” structure that each reduces the spatial resolution by a factor of 2. After that, there are a total of L layers with unknown spatial resolutions, with the maximum being downsampled by 4 and the minimum being downsampled by 32. Since each layer may differ in spatial resolution by at most 2, the first layer after the stem could only be either downsampled by 4 or 8. We illustrate our network level search space in Fig. 1. Our goal is then to find a good path in this L-layer trellis. 这篇论文，在内部核单元搜索空间部分，重用了之前工作中的搜索空间，而在外部网络结构部分提出了一个新的基于对一些流行的设计的观察和总结而得出的搜索空间。 外部网络的搜索空间，这个网络的开始是一个两层的茎干部分，逐个减少两倍空间分辨率，在此之后是一个L层的未知空间分辨率，最大的是从4开始降采样，最小的是从32开始降采样。这个网络如图一所示，目标是在这个L层的格子里找到一个合适的路径。 Methods The advantage of introducing this continuous relaxation is that the scalars controlling the connection strength between different hidden states are now part of the differentiable computation graph. Therefore they can be optimized efficiently using gradient descent. We adopt the first-order approximation and partition the training data into two disjoint sets trainA and trainB. We decode the discrete cell architecture by first retaining the 2 strongest predecessors for each block , and then choose the most likely operator by taking the argmax. Quite intuitively, our goal is to find the path with the “max- imum probability” from start to end. This path can be decoded efficiently using the classic Viterbi algorithm, as in our implementation. 这篇论文先通过公式将核架构和网络架构进行连续化操作，这样两层架构均变成了可微分的计算图，进而可以使用梯度下降的方法对其进行优化。针对核架构的解码，通过保留每一块最优的两个前置参数，并通过argmax选择最可能的运算符。针对外部网络架构的编码，目的是要找到一条从开始到结束的最大路径，这个路径可以使用经典的Viterbi算法编码。 Experimental Results We consider a total of L = 12 layers in the network, and B = 5 blocks in a cell. The network level search space has 2.9 × 104 unique paths, and the number of cell structures is 5.6 × 1014. So the size of the joint, hierarchical search space is in the order of 1019. The architecture search optimization is conducted for a total of 40 epochs. The batch size is 2 due to GPU mem- ory constraint. When learning network weights w, we use SGD optimizer with momentum 0.9, cosine learning rate that decays from 0.025 to 0.001, and weight decay 0.0003. The initial values of α, β before softmax are sampled from a standard Gaussian times 0.001. They are optimized using Adam optimizer [36] with learning rate 0.003 and weight decay 0.001. On Cityscapes, Auto-DeepLab significantly outperforms the previous state-of-the-art by 8.6%, and per- forms comparably with ImageNet-pretrained top models when exploiting the coarse annotations. On PASCAL VOC 2012 and ADE20K, Auto-DeepLab also outperforms several ImageNet-pretrained state-of-the-art models. 将NAS使用在语义分割上，经过在不同数据集上的实验，可以得到：在Cityscapes数据集上，auto-deepLab 明显的比其他一些经过ImageNet预训练的先进的方法表现良好，而且和经过ImageNet预训练的顶级模型相比性能差不多；在PASCAL VOC2012和ADE20K的数据集上，auto-deepLab和一部分经过ImageNet预训练的先进模型效果差不多，但表现效果并不是最好的。 Conclusion For future work, within the current framework, related applications such as object detection should be plausible; we could also try untying the cell architecture α across different layers with little computation overhead. Beyond the current framework, a more general network level search space should be beneficial. 这种将核架构搜索和外部网络架构搜索一同使用的NAS是一个较好的发展方向，未来也可以应用于其他相关领域，比如物体检测。此外也可以从减少计算资源的角度着手，这样可以使得NAS有更广泛的应用场景。 Learning Transferable Architecture for Scalable Image Recognition Zoph, B., Vasudevan, V., Shlens, J., &amp; Le, Q. V. (2018). Learning Transferable Architectures for Scalable Image Recognition. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 8697–8710. https://doi.org/10.1109/CVPR.2018.00907 Introduction In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset. We design a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. Our main result is that the best architecture found on CIFAR-10, called NASNet. The image features learned by NASNets are generically useful and transfer to other computer vision problems. 这篇论文提出了一中新的设计卷积网络架构的方法，并且优化了一些基于数据集的网络架构，设计了一个搜索空间，这个搜索空间被称为NASNet搜索空间。这种搜索空间使网络架构的复杂度独立于网络的深度和输入图片的大小。另外，这篇论文的主要成果就是在CIFAR-10数据集上得到的最优架构，即NASNet。从NASNet中提取出的图片特征一般均能转移到其他的计算机视觉问题。 Related Work The proposed method is related to previous work in hyperparameter optimization. The design of our search space took much inspiration from LSTMs, and Neural Architecture Search Cell. 这篇论文和之前的一些超参数优化的工作相关，搜素空间的设计则来源于LSTM和NAS单元。 Method The main contribution of this work is the design of a novel search space, such that the best architecture found on the CIFAR-10 dataset would scale to larger, higher- resolution image datasets across a range of computational settings. In our approach, the overall architectures of the convolutional nets are manually predetermined. They are composed of convolutional cells repeated many times where each convolutional cell has the same architecture, but different weights. To easily build scalable architectures for images of any size, we need two types of convolutional cells to serve two main functions when taking in a feature map as input: (1) convolutional cells that return a feature map of the same dimension, and (2) convolutional cells that return a feature map where the feature map height and width is reduced by a factor of two. We name the first type and second type of convolutional cells Normal Cell and Reduction Cell respectively. The Reduction and Normal Cell could have the same architecture, but we empirically found it beneficial to learn two separate architectures. In our search space, each cell receives as input two initial hidden states hi and hi−1 which are the outputs of two cells in previous two lower layers or the input image. The controller RNN recursively predicts the rest of the structure of the convolutional cell, given these two initial hidden states(Figure 3). 在这篇论文的方法中，全部的卷积网络的架构是人工预先定义的，这些架构是由重复多次的卷积核组成，这些卷积核具有相同的结构和不同的权重。为了能够方便地构建针对任意大小图片的可拓展架构，这里提供了两种类型的卷积核，对于输入的特征图片分别对应不同的功能。一种能返回同样维度的特征图谱，称为Normal核；一种能够让特征图谱的宽高减半，称为Reduction核。这两种卷积核可以有同样的架构，但经过实验发现，不同的架构效果更好。 在搜索空间中，每一个卷积核以两个初始隐藏层状态作为两个输入参数，这两个参数要么是两个卷积核的输出，要么是输入的图像信息。控制器循环网络基于这两个参数递归预测卷积核剩下的结构。 Experiments and Results As can be seen from the Table 1, a large NASNet-A model with cutout data augmentation [12] achieves a state-of-the-art error rate of 2.40% (averaged across 5 runs), which is slightly better than the previous best record of 2.56% by [12]. We find that the learned convolutional cells are flexible across model scales achieving state-of-the-art performance across almost 2 orders of magnitude in computational budget. Increasing the spatial resolution of the input image results in the best reported, single model result for object detection of 43.1%, surpassing the best previous best by over 4.0% . NASNet provides superior, generic image features that may be transferred across other computer vision tasks. The best model identified with RL is significantly better than the best model found by RS by over 1% as measured by on CIFAR-10. 这篇论文在CIFAR-10和ImageNet数据集上进行了图像分类实验，得出的结果比之前的一些先进的方法略优一些。另外还将在ImageNet上预训练好的NASNet网络移入faster-rcnn框架进行了物体检测实验，数据集是COCO数据集，结果表现性能也不错，说明了这个架构可以迁移来解决到其他计算机视觉问题。最后还用实验说明了，使用强化学习（RL）进行架构搜索得到的模型结果比随机搜索（RS）得到的模型结果表现好。 Conclusion The learned architecture is quite flexible as it may be scaled in terms of computational cost and parameters to easily address a variety of problems. The key insight in our approach is to design a search space that decouples the complexity of an architecture from the depth of a network. we demonstrate that we can use the resulting learned architecture to perform ImageNet classification with reduced computational budgets that outperform streamlined architectures targeted to mobile and embedded platforms. 这篇论文提出的NASNet能够很灵活地可以扩展到计算机视觉的其他问题的解决上，而且NASNet搜索空间也从网络深度层面增加了搜索空间的复杂度。 Random Search and Reproducibility for Neural Architecture Search Li, L., &amp; Talwalkar, A. (2019). Random Search and Reproducibility for Neural Architecture Search. 1–20. Retrieved from http://arxiv.org/abs/1902.07638 Introduction We see three fundamental issues with the current state of NAS research: Inadequate Baselines. Complex Methods. Lack of Reproducibility. We help ground existing NAS results by providing a new perspective on the gap between traditional hy- perparameter optimization and leading NAS methods. We identify a small subset of NAS components that are sufficient for achieving good empirical results. We open-source all of the necessary code, random seeds, and documentation necessary to reproduce our experiments. 这篇论文发现了现存的NAS研究里的三种基本的问题，分别是：基准不足，方法复杂，难以复现。 这篇论文的贡献是：在传统的超参数调优方法和先进的NAS方法的比较中提出了新的看法；从NAS组件中选出了一部分足以实现好的实验效果的子集；开源了必要的代码，随机种子和复现实验时所需文档。 Related Work We choose to use a simple method combining random search with early-stopping called ASHA to provide a competitive baseline for standard hyperparameter optimization. Our combination of random search with weight-sharing greatly simplifies the training routine and we identify key variables needed to achieve competitive results on both CIFAR-10 and PTB benchmarks. We follow DARTS and report the result of our random weight-sharing method across multiple trials; in fact, we go one step further and evaluate the broad reproducibility of our results with multiple sets of random seeds. 在相关工作这一部分，这篇论文针对NAS研究里的三个问题分别列举了当前的一些研究工作，并给出了自己相应的解决方案。关于基准不足，这篇论文使用了结合早停的随机搜索的方法，这个方法给标准的超参数调优提供了一个有竞争力的基准；关于方法复杂，这篇论文使用了结合参数共享的随机搜索方法，进而简化了训练过程；关于难以复现，这篇论文给出了复现的相关文档和随机种子，并对结果的可复现性进行了评测。 Methodology Our algorithm is designed for an arbitrary search space with a DAG representation, we use the same search spaces as that considered by DARTS [34] for the standard CIFAR-10 and PTB NAS benchmarks. There are a few key meta-hyperparameters that impact the behavior of our search algorithm: Training epochs.Batch size.Network size.Number of evaluated architectures. Since we train the shared weights using a single architecture at a time, we have the option of only loading the weights associated with the operations and edges that are activated into GPU memory. Hence, the memory footprint of our random search with weight-sharing can be reduced to that of a single model. 这篇论文的随机搜索算法使用的搜索空间和DARTS在CIFAR-10和PTB上使用的搜索空间相同。另外，有一些元超参数能够影响这篇论文的搜索算法，比如训练次数，分块大小，网络大小和评估架构的数量。最后，因为这篇论文在训练共享权重时每次只使用一个架构，所以每次只需要将和架构相关的权重和操作加载进GPU存储里就行，因此这里的结合权重共享的随机搜索算法能够节省GPU开销。 Experiments To evaluate the performance of random search with weight-sharing on these two benchmarks, we proceed in the same three stages: Stage 1: Perform architecture search for a cell block on a cheaper search task. Stage 2: Evaluate the best architecture from the first stage by retraining a larger, network formed from multiple cell blocks of the best found architecture from scratch. This stage is used to select the best architecture from multiple trials. Stage 3: Perform the full evaluation of the best found architecture from the second stage by either training for more epochs (PTB) or training with more seeds (CIFAR-10). For the PTB benchmark, we refer to the network used in the first stage as the proxy network and the network in the later stages as the proxyless network. We next present the final search results. We subsequently explore the impact of various meta-hyperparameters on random search with weight-sharing, and finally evaluate the reproducibility of various methods on this benchmark. Comparison with state-of-the-art NAS methods and manually designednetworks. Lower test perplexity is better on this benchmark. For the CIFAR-10 benchmark,the DAG considered for the convolutional cell has N = 4 search nodes and the operations considered include 3 × 3 and 5 × 5 separable convolutions, 3 × 3 and 5 × 5 dilated separable convolutions, 3 × 3 max pooling, and 3 × 3 average pooling, and zero.Comparison with state-of-the-art NAS methods and manually designed networks. 为了评估在在PTB和CIFAR-10两个基准上权重共享的随机搜索的性能，这篇论文进行了相同的三个阶段: 阶段1:在一个资源耗费较少的搜索任务上对一个单元格块执行架构搜索。 阶段2:通过重新训练一个更大的网络来评估第一个阶段的最佳架构，这个网络由多个单元块组成，这些单元块是从零开始找到的最佳架构。这个阶段用于从多个测试中选择最佳架构。 阶段3:对第二阶段发现的最佳体系结构进行全面评估，方法是进行更多epoch (PTB)的训练，或者进行更多seed (CIFAR-10)的训练。 对在两个基准上分别进行上述三个阶段的操作，最终对比一些已有的NAS方法和人工调参，得到上述两个对比表，可以看出，在PTB基准上能够得到比其他方法较小的perplexity，而在CIFAR-10基准上能够得到相对较少的errors。 Conclusion Better baselines that accurately quantify the performance gains of NAS methods. Ablation studies that isolate the impact of individual NAS components. Reproducible results that engender confidence and foster scientific progress. 这篇论文成果有，提出了一个基本的基准（即论文中随机搜索的方法）来准确地衡量NAS方法的性能收益；对多个NAS组件的影响进行了探索；使NAS实验结果可复现，推动了科研进程。]]></content>
      <categories>
        <category>Paper Notes</category>
      </categories>
      <tags>
        <tag>AutoML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络结构搜索综述笔记]]></title>
    <url>%2F2019%2F08%2F14%2FNASSurveyNotes%2F</url>
    <content type="text"><![CDATA[NAS_Survey_Notes_Ⅰ Elsken, T., Metzen, J. H., &amp; Hutter, F. (2019). Neural Architecture Search. 20, 63–77. https://doi.org/10.1007/978-3-030-05318-5_3 First Section: Introduction​ NAS (Neural Architecture Search) methods can be categorized to three dimensions: search space, search strategy, and performance estimation strategy. A search strategy selects an architecture A from a predefined search space の，The architecture is passed to a performance estimation strategy, which returns the estimated performance of A to the search strategy. Search Space: The search space defines which architectures can be represented in principle. Search Strategy: The search strategy details how to explore the search space (which is often exponentially large or even unbounded) Performance Estimation Strategy: Performance Estimation refers to the process of estimating this performance: the simplest option is to perform a standard training and validation of the architecture on data, but this is unfortunately computationally expensive and limits the number of architectures that can be explored. Much recent research therefore focuses on developing methods that reduce the cost of these performance estimations. NAS(Neural Architecture Search)神经网络结构搜索方法可以分为三个维度：搜索空间，搜索策略和性能评估策略 搜索策略通过搜索预先定义好的搜索空间，获得候选结构，然后候选结构经过性能评估策略的评估，返回其对应的评估性能，之后反复迭代上述过程，直到选出符合要求性能的网络结构。 搜索空间定义了理论上能够表示的神经网络结构。 搜索策略是指如何探测搜索空间（搜索空间比较大，甚至没有边界） 性能评估策略，则是评估候选网络结构的性能，最简单的方式就是训练和验证这个网络结构，但这样会耗费大量的计算，而且比较耗时，所以需要探索新的方法来减少性能评估的时间和计算开销。 Second Section: Search SpaceThe space of chain-structured neural networks is a relatively simple search space. It is parameterized by (Ⅰ)the (maximum) number of layers n (possibly unbounded); (Ⅱ)the type of operation every layer executes; (Ⅲ)hyperparameters associated with the operation. The space of multi-branch architectures is complex search space. As for this multi-branch architectures , we can search for such motifs, dubbed cells or blocks, respectively, rather than for whole architectures. Zoph et al. (2018) optimize two different kind of cells: a normal cell that preserves the dimensionality of the input and a reduction cell which reduces the spatial dimension. The final architecture is then built by stacking these cells in a predefined manner. This cell-based search space has three major advantages compared with the whole search space: The size of the search space is drastically reduced since cells usually consist of significantly less layers than whole architectures. Architectures built from cells can more easily be transferred or adapted to other data sets by simply varying the number of cells and filters used within a model. Creating architectures by repeating building blocks has proven a useful design principle in general. However, a new design-choice arises when using a cell-based search space, namely how to choose the macro-architecture: how many cells shall be used and how should they be connected to build the actual model?And one direction of optimizing macro-architectures is the hierarchical search space. 链结构神经网络的空间是一个相对简单的搜索空间。它的参数由以下三部分组成：（Ⅰ）（最大）层数n（可能无界）; （二）每层执行的操作类型; （Ⅲ）与操作相关的超参数。 多分支架构的空间是复杂的搜索空间。对于这种多分支架构，我们可以分别搜索单元格或块，而不是整个架构。 Zoph等人。 （2018）优化两种不同类型的单元：保持输入维度的正常单元和减小空间维度的还原单元。然后通过以预定义的方式堆叠这些单元来构建最终的体系结构。与整个搜索空间相比，这种基于单元的搜索空间具有三大优势： 1.搜索空间的大小大大减少，因为单元的层数往往比整个体系结构层数少得多。2.通过简单地改变模型中使用的单元和过滤器的数量，可以更容易地将从单元构建的架构转移或适应其他数据集。3.通过重复构建块来创建体系结构已经被证明是一种有用的设计原则。 然而，当使用基于单元的搜索空间时，出现了一种新的设计选择，即如何选择宏架构：应该使用多少个单元以及如何连接它们来构建实际模型？优化宏架构的一个方向是使用分层搜索空间。 Third Section: Search StrategyMany different search strategies can be used to explore the space of neural architectures, including random search, Bayesian optimization, evolutionary methods, reinforcement learning (RL), and gradient-based methods. Real et al. (2019) conduct a case study comparing RL, evolution, and random search (RS), concluding that RL and evolution perform equally well in terms of final test accuracy, with evolution having better anytime performance and finding smaller models. Both approaches consistently perform better than RS in their experiments, but with a rather small margin: RS achieved test errors of approximately 4% on CIFAR-10, while RL and evolution reached approximately 3.5%. 许多不同的搜索策略可用于探索神经架构的空间，包括随机搜索，贝叶斯优化，进化方法，强化学习（RL）和基于梯度的方法。 Real等人（2019）比较RL，进化算法和随机搜索（RS）这三个算法，得出结论：RL和进化算法在最终测试准确性方面表现同样良好，并且进化算法具有更好的性能，而且找到更小的模型。 两种方法在实验中始终表现优于RS。 Fourth Section: Performance Estimation Speed-up Strategy Speed-up method How are speed-ups achieved? Lower fidelity estimates低保真估计 Training time reduced by training for fewer epochs, on subset of data, downscaled models, downscaled data.通过训练更少的阶段，数据子集，缩减模型，缩减的数据来减少训练时间。 Learning Curve Extrapolation学习曲线外推 Training time reduced as performance can be extrapolated after just a few epochs of training在几个训练阶段完成之后推断根据学习曲线推断模型性能进而减少训练时间 Weight Inheritance / Network Morphisms权重继承或网络态射 Instead of training models from scratch, they are warm-started by inheriting weights并不是从头开始训练模型，而是继承了权重，可以减少训练次数 One-shot Models/Weight Sharingone-shot模型或权重共享 Only the one-shot model needs to be trained; its weights are then shared across different architectures that are just subgraphs of the one-shot model.只需要训练一个one-shot模型，它的权重被不同的结构所使用，这些不同的结构可以看作是one-shot模型的子图 Appendix Ⅰ : This Survey Mind map]]></content>
      <categories>
        <category>Paper Notes</category>
      </categories>
      <tags>
        <tag>AutoML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化机器学习综述笔记]]></title>
    <url>%2F2019%2F08%2F06%2FAutoMLSurveyNotes%2F</url>
    <content type="text"><![CDATA[Auto Machine Learning Survey Notes(Ⅰ) Paper Name: Yao, Q., Wang, M., Chen, Y., Dai, W., Yi-Qi, H., Yu-Feng, L., … Yang, Y. (2018). Taking Human out of Learning Applications: A Survey on Automated Machine Learning. 1–26. Retrieved from http://arxiv.org/abs/1810.13306 First Section In first section,this paper first explains why we need to investigate AutoML. Its reason is following. Machine Learning has been popular recently , but every aspect of machine learning applications, such as feature engineering, model selection, and algorithm selection, needs to be carefully configured, which costs the efforts of many human experts. So we need to research AutoML to take human out of Learning Application and give experts more time to analyze data and problem,and finally promote the development of Machine Learning. This paper also gives some examples of automated machine learning in industry and academic, like Auto-sklearn, Google’s cloud autoML , and Feature Labs, which shows that AutoML has already been successfully applied in many important problems. 在第一部分中，本文首先解释了为什么我们需要研究AutoML。 因为机器学习近些年很流行，但机器学习应用的各个方面，如特征工程，模型选择和算法选择，都需要仔细配置，这需要许多专业人士的努力。所以我们需要研究AutoML以使人类能够从模型训练中解放出来，能够集中精力分析数据和问题本身，最终促进机器学习的发展。 本文还提供了一些工业和学术界自动化机器学习的例子，如Auto-sklearn，Google的 Cloud AutoML，还有Feature Labs，它们表明了AutoML已成功应用于许多重要问题。 Second SectionIn second section, this paper firstly defines AutoML and its core goal. AutoML attempts to construct machine learning programs(specified by Experience, Task and Performance ),without human assitance and within limited computational budgets. And its three core goals are good performance , no assistance from humans and high computational efficiency. Next, this paper propose a basic framework for AutoML approaches. In this framework, an AutoML controller, which has two key ingredients,the optimizer and the evaluator, takes the place of human to find proper configurations for the learning tools. The duty of the evaluator is to measure the performance of the learning tools with configurations provided by the optimizer , while the optimizer’s duty is to update or generate configurations for learning tools. Finally, this paper shows AutoML approaches taxonomies by problem setup and techniques. 在第二部分中，本文首先定义了AutoML及其核心目标。 AutoML尝试构建机器学习程序（由经验，任务和性能指定），无需人工协助并且在有限的计算预算内。 它的三个核心目标是良好的性能，没有人类的帮助和高计算效率。 接下来，本文提出了AutoML方法的基本框架。 在这个框架中，AutoML控制器取代了人类的工作，为学习方法找到合适的配置方式，这个控制器有两个关键组成部分，即优化器和评估器。 在本节的最后，本文将AutoML的方法分为问题设置和技术处理两类。 Third SectionIn third section, this paper gives details on problem setup and introduces some existing AutoML approaches on learning process. AutoML approaches do not necessarily cover the full machine learning pipeline, they can also focus on some parts of the learning process. As for some parts of the learning process, AutoML approaches includes feature engineering , model selection, and optimization algorithm selection. Feature engineering is to automatically construct features from the data so that subsequent learning tools can have good performance. This goal can be divided into two sub-problems. creating features from the data and enhance features’ discriminative ability. However, there are no common or principled methods to create features from data. So , now AutoML focus on feature enhancing methods and there are some common methods to enhance features, dimension reduction,feature generation and feature encoding. Models selection contains two components, picking up some classifiers and setting their corresponding hyper-parameters. The task of model selection is to automatically select classifiers and set their hyper-parameters so that good learning performance can be contained. The goal of optimization algorithm selection is to automatically find an optimization algorithm so that efficiency and performance can be balanced. What’s more, in this section, this paper give two classes of full-scope AutoML approaches. The first one is general case, which is a combination of feature engineering, model selection and algorithm selection. The second one is Network Architecture Search (NAS), which targets at searching good deep network architectures that suit the learning problem. 在第三部分中，本文详细介绍了问题的设置，并介绍了一些现有的AutoML的方法。AutoML可以不必覆盖机器学习的全范围，仅专注于学习过程中的一部分。 针对机器学习的某些部分，AutoML的方法可分为特征工程，模型选择和优化算法选择。特征工程是从数据中自动构造特征，以便后续学习工具可以具有良好的性能。这个目标可以分为两个子问题，一个是从数据创建功能，另一个是增强功能的辨别能力。但是，没有通用或原则方法从数据创建功能。因此，现在AutoML专注于特征增强方法，并且有一些常用方法可以增强特征，例如降维，特征生成和特征编码。模型选择包含两个组成部分，分别是选取分类器和设置其相应的超参数。模型选择的任务是自动选择分类器并设置其超参数，以便可以包含良好的学习性能。优化算法选择的目标是自动找到优化算法，以便平衡效率和性能。 此外，在本节中，本文提供了两类全范围AutoML方法。第一个是一般情况，它是特征工程，模型选择和算法选择的组合。第二个是网络架构搜索（NAS），其目标是搜索适合学习问题的良好深度网络架构。 Fourth SectionIn fourth section, this paper introduces some basic techniques for optimizer. It has three parts including simple search approaches, optimization from samples and gradient descent. Simple search is a naive search approach, and grid search and random search are two common approaches. Optimization from samples is a kind of smarter search approach, and this paper divide existing approaches into three categories, heuristic search, model-based derivative-free optimization, and reinforcement learning. Some popular heuristic search methods are Particle swarm optimization(PSO), Evolutionary Algorithms. The popular methods of model-based derivative-free optimization are Bayesian optimization, classification-based optimization (CBO) and simultaneous optimistic optimization (SOO) . Reinforcement learning (RL) is a very general and strong optimization framework, which can solve problems with delayed feedbacks. Greedy search is a natural strategy to solve multi-step decision-making problem. 在第四部分中，本文介绍了优化器的一些基本技术。 它有三个部分，包括简单搜索方法，样本优化和梯度下降。 简单搜索是一种朴素的搜索方法，其中两种常见的方法分别是网格搜索和随机搜索。样本优化是一种更智能的搜索方法，本文将现有的相关方法分为三类：启发式搜索，基于模型的无导数优化和强化学习。 一些流行的启发式搜索方法是粒子群优化（PSO），进化算法。 基于模型的无导数优化的流行方法是贝叶斯优化，基于分类的优化（CBO）和同时优化优化（SOO）。 强化学习（RL）是一个非常通用且强大的优化框架，可以解决延迟反馈的问题。此外，贪心搜索是解决多步决策问题的常用策略。 Fifth SectionIn fifth section, this paper introduce some basic techniques for evaluator. And the existing methods are direct evaluation, sub-sampling, early stop, parameter reusing and surrogate evaluator. Direct evaluation is often accurate but expensive, and some other methods have been proposed for acceleration by trading evaluation accuracy for efficiency. 在第五部分，本文介绍了评估器的一些基本技术。 现有的方法有直接评估，子抽样，早期停止，参数重用和代理评估。 直接评估通常是准确的但效率不高，而其他的方法是来通过降低评估准确性来提高效率。 Sixth SectionIn sixth section, this paper introduce some experienced techniques , there are two main topics, meta-learning and transfer learning. Meta-learning helps AutoML, on the one hand, by characterizing learning problems and tools.,on the other hand, the meta-learner encodes past experience and acts as a guidance to solve future problems. Existing meta-learning techniques by categorizing them into three general classes based on their applications in AutoML are following: meta-learning for configuration evaluation (for the evaluator). Meta-learners can be trained as surrogate evaluators to predict performances, applicabilities, or ranking of configurations. Representative applications of meta-learning in configuration evaluation are Model evaluation and General configuration evaluation. meta- learning for configuration generation (for the optimizer). The approaches have promising configuration generation, warming-starting configuration generation , and search space refining. meta-learning for dynamic configuration adaptation. Meta-learning can help to automate this procedure by detecting concept drift and dynamically adapting learningdrift. Transfer learning tries to improve the learning on target domain and learning task, by using the knowledge from the source domain and learning task. Transfer learning has been exploited to reuse trained surrogate models or promising search strategies from past AutoML search (source) and improve the efficiency in current AutoML task (target). By transferring knowledge from previous configuration evaluations, we can avoid training model from scratch for the upcoming evaluations and significantly improve the efficiency. 在第六部分，本文介绍了一些比较先进的技术，主要有两个主题，元学习和迁移学习。 元学习辅助AutoML，一方面通过对学习问题和工具的表征。另一方面，通过编码以往的经验来作为对解决未来问题的指导。 基于AutoML中的应用程序，将现有的元学习技术分为三个通用类： 1.用于配置评估的元学习（用于评估器）。元学习器可以作为代理评估器进行训练，以预测性能，适用性或配置排名。元学习在配置评估中的代表性应用有模型评估和一般配置评估这两种。2.用于配置生成的元学习（用于优化器）。相关方法有：良好配置生成，预热配置生成和搜索空间精炼。3.动态配置适应的元学习。元学习可以通过检测概念转换和动态调整学习转换来帮助AutoML。 迁移学习通过使用来自源域和学习任务的知识，尝试改进对目标域和学习任务的学习。 迁移学习能重用过去AutoML任务（源）经过训练的代理模型或优良的搜索策略，并提高当前AutoML任务（目标）的效率。通过从以前的配置评估中迁移知识，可以避免从头训练模型，显著提高效率。 Seventh SectionIn seventh section, this paper introduce three AutoML examples, Auto-sklearn, NASNet and ExploreKit. In Auto-sklearn, model selection is formulated as a CASH problem, which aims to minimize the validation loss with respect to the model as well as its hyper-parameters and parameters. RL is employed in NAS to search for a optimal sequence of design decisions. Besides, the direct evaluation is used in these works as the evaluator. As RL is slow to converge, to make the search faster, transfer learning, which is firstly used to cut the search space into several blocks. ExploreKit conducts more expensive and accurate evaluations on the candidate features. Error reduction on the validation set is used as the metric for feature importance. Candidate features are evaluated successively according to their ranking, and selected if their usefulness surpass a threshold. This procedure terminates if enough improvement is achieved. The selected features will be used to generate new candidates in the following iterations. 在第七部分中，本文介绍了三个AutoML应用示例，Auto-sklearn，NASNet和ExploreKit。 在Auto-sklearn中，模型选择被公式化为CASH问题，其目的是最小化模型的验证损失函数及其超参数和参数。 NAS使用RL来搜索最佳的设计决策序列，使用直接评估作为整个工作的评估器。但RL收敛缓慢，为了使搜索更快，NAS使用了转移学习，在搜索之前将搜索空间切割成几个块，以加快收敛速度。 ExploreKit对候选功能进行更准确但更低效的评估。将验证集上的错误减少情况用作特征有效性的度量标准，按照相应排名对候选特征进行评估，并选择有效性超过阈值的特征，所选的特征将在以下的迭代中生成新候选。如果实现了足够的效果，则迭代过程终止。 Eighth SectionIn eighth section, this paper reviews the history of AutoML, summarizes how its current status in the academy and industry, and discuss its future work. AutoML only becomes practical and a big focus recently, due to the big data, the increasing computation of modern computers, and of course, the great demand of machine learning application. AutoML is a very complex problem and also an extremely active research area, and there are many new opportunities and problems in AutoML. And there are also many workshops and competitions. Higher efficiency can be achieved by either proposing algorithms for the optimizer, which visit less configurations before reaching a good performance, or designing better methods for the evaluator, which can offer more accurate evaluations but in less time. 在第八部分，本文回顾了AutoML的历史，总结了其在学术界和行业中的现状，并讨论了其未来的发展方向。 由于大数据，现代计算机的计算量不断增加，当然还有机器学习应用的巨大需求，AutoML最近才成为重点。 AutoML是一个非常复杂的问题，也是一个非常活跃的研究领域，AutoML中存在许多新的机会和问题，还有许多研讨会和比赛。 为优化器提出算法可以实现更高的效率，这样使得优化器在达到良好性能之前访问较少的配置；或者为评估器设计更好的方法，这可以在更短的时间内提供更准确的评估。 Appendix Ⅰ : This Survey Mindmap Auto Machine Learning Survey Notes(Ⅱ) Elshawi, R., Maher, M., &amp; Sakr, S. (2019). Automated Machine Learning: State-of-The-Art and Open Challenges. Retrieved from http://arxiv.org/abs/1906.02287 First SectionIn first section, this paper briefly introduces the development of AutoML and explain what is the CASH (Combined Algorithm Selection and Hyper-parameter tuning) problem. 在第一部分中，本文简要介绍了AutoML的发展，并解释了什么是CASH问题。 Second SectionIn Second section, this paper introduces the various techniques that have been introduced to tackle the challenge of warm starting(meta-learning) for AutoML search problem in the context of machine learning and deep learning domains.. These techniques can generally be categorized into three broad groups: learning based on task properties, learning from previous model evaluations and learning from already pretrained models. 在第二部分中，本文介绍了在机器学习和深度学习领域中针对AutoML搜索问题的热启动（元学习）挑战所引入的各种技术。 这些技术通常可以分为三大类：从任务属性中学习，从先前模型评估中学习以及从已经预训练的模型中学习。 Third SectionIn third section, this paper introduces the various approaches that have been introduced for tackling the challenge of neural architecture search(NAS) in the context of deep learning. Broadly, NAS techniques falls into five main categories including random search, reinforcement learning, gradient- based methods, evolutionary methods, and Bayesian optimization. 在第三部分中，本文介绍了在深度学习环境中应对神经结构搜索（NAS）挑战的各种方法。 从广义上讲，NAS技术分为五大类，包括随机搜索，强化学习，基于梯度的方法，进化方法和贝叶斯优化。 Fourth SectionIn fourth section, this paper introduces some different approaches for automated hyper-parameter optimization. In principle, the automated hyper-parameter tuning techniques can be classified into two main categories: black-box optimization techniques and multi-fidelity optimization techniques. 在第四部分中，本文介绍了一些不同的自动超参数优化的方法。 原则上，自动超参数调整技术可以分为两大类：黑盒优化技术和多保真优化技术。 Fifth SectionIn fifth section, this paper covers the various tools and frameworks that have been implemented to tackle the CASH problem. In general, these tools and frameworks can be classified into three main categories: centralized, distributed, and cloud-based. And Neural Network Automation Frameworks is also a class recently. 在第五部分中，本文介绍了为解决CASH问题而实施的各种工具和框架。 这些工具和框架可以分为三大类：集中式框架，分布式框架和基于云平台的框架。此外，神经网络自动化框架也逐渐发展起来。 Sixth SectionIn sixth section, this paper introduces some state-of-the-art research efforts on tackling the automation aspects for the other building blocks (Pre-modeling and Post-Modeling) of the complex machine learning pipeline. 在第六部分中，本文介绍了一些关于自动化复杂机器学习问题中的其他构建模块（比如预建模和后建模）的前沿研究工作，。 Seventh SectionIn seventh section, this paper introduces some research directions and challenges that need to be addressed in order to achieve the vision and goals of the AutoML process. It includes scalability, optimization techniques, time budget, composability, user friendliness, continuous delivery pipeline, data validation, data preparation, and model deployment and life cycle. 在第七部分中，本文介绍了为了实现AutoML过程的这个目标，需要进一步研究的的一些方向和挑战。它包括可扩展性，优化技术，时间预算，可组合性，用户友好性，持续交付管道，数据验证，数据准备，模型部署和生命周期。 Appendix Ⅰ : This Survey Mind map Auto Machine Learning Survey Notes(Ⅲ) Zöller, M.-A., &amp; Huber, M. F. (2019). Survey on Automated Machine Learning. Retrieved from http://arxiv.org/abs/1904.12054 First SectionIn first section, this paper introduces the history of AutoML, and shows AutoML is no new trend. It also gives this paper’s contributions: introduce a mathematical formulation covering the complete procedure of automatic ML pipeline creation, cover AutoML techniques for each step of building an ML pipeline, and evaluate all presented algorithms. Finally, it shows this paper’s structure. 在第一部分中，本文介绍了自动化机器学习的发展历史，并指出它并不是一个新的趋势。本文还给出了这篇论文的贡献：介绍了一个覆盖整个自动化机器学习过程的数学公式，介绍了构建机器学习流水线的每一步的automl技术，并对所提出的所有算法进行了评估。最后给出了这篇论文的脉络结构。 Second SectionIn second section, this paper gives a mathematical sound formulation of the automatic construction of ML pipelines is given. And it shows most current state-of-the-art algorithms solve the pipeline creation problem in two distinct stages, pipeline structure search, and Algorithm selection and hyperparameters optimization. 第二部分给出了ML流水线自动构造的数学合理公式。并说明了目前最先进的算法如何解决了流水线创建问题，分为两个子问题，分别是流水线结构搜索和算法选择及超参数优化。 Third SectionIn third section, this paper introduces some approaches about pipeline structure creation. a basic ML pipeline layout: At first, the input data is cleaned in multiple distinct steps, like imputation of missing data and one-hot encoding of categorical input. Next, relevant features are selected and new features created in a feature engineering stage. This stage highly depends on the underlying domain. Finally, a single model is trained on the previously selected features. a fixed pipeline shape： Fixed ML pipeline used by most AutoML frameworks. Minor differences exist regarding the implemented data cleaning steps. Regarding data cleaning, the pipeline shape differs. Yet, often the two steps imputation and scaling are implemented. Even though this approach greatly reduces the complexity of the pipeline creation problem, it leads to inferior pipeline performances for complex data sets. Fixed shaped ML pipelines lack this flexibility to adapt to a specific task. some variable shapes: genetic programming, Genetic programming is an iterative, domain-agnostic optimization method derived from biological evolution. hierarchical task networks(HTNs)，HTNs are a method from automated planning that recursively partition a complex problem into easier subproblems. self-play, Self-play (Lake et al., 2017) is a reinforcement learning strategy, the algorithm creates new training examples by playing against itself. A common drawback of self-play approaches is the low convergence speed making this approach rather unsuited for AutoML. 第三部分介绍了一些关于机器学习流水线创建的方法。 首先介绍了一个基本的机器学习流水线布局，在这个布局里，输入数据先被清洗，然后从中提取出特征，最后，根据提取出的特征训练模型。 之后，介绍了固定的流水线布局，这个布局被大部分自动化机器学习框架采用。这个布局与基本的机器学习流水线布局相比，只有在数据清洗的步骤有所差异。在该布局中，数据清洗是由两个步骤完成的，插补和缩放。尽管这种布局极大地减少了流水线创建的复杂度，但它也使得流水线在处理复杂的数据集时性能不高。而且，这种布局还缺乏适应特定任务的灵活性。 最后，介绍了一些流水线布局的变体。第一个是基于遗传编码的变体，这种方法是一中从生物进化借鉴而来的迭代优化方法；第二个是基于层次任务网络的方法，这种方法是一种自动地将复杂问题递归地划分为简单问题的方法。第三个是self-play方法，这个是一种强化学习的策略，这个方法通过与自身对抗创建新的训练示例，但是这个方法的一个常见缺点就是收敛速度低，因此它不是很适合autoML。 Fourth SectionIn fourth section, this paper shows some approaches to solve CASH problem (the combined algorithm selection and hyperparameter optimization, both steps are executed simultaneously) . Grid Search, The first approach proposed to systematically explore the configuration space was grid search. grid search creates a grid of configurations and evaluates all of them. Therefore, each continuous hyperparameter is discretized into k (logarithmic) equidistant values. This basic algorithm does not scale well for large configuration spaces, as the number of function evaluations grows exponentially with the number of hyperparameters. Random Search, A candidate configuration is generated by randomly choosing a value for each hyperparameter independently of all others. Random search is straightforward to implement and parallelize and well suited for gradient-free functions with many local minima . Sequential Model-Based Optimization: Gaussian Process, Even though a Gaussian process is a non-parametric model, it still requires hyperparameters itself, namely the selection of m and k. A common drawback of Gaussian processes is the runtime complexity of O(n3) due to the inversion of K. Random Forests, Random forest regression is an ensemble method consisting of multiple regression trees Tree-structured parzen estimator, TPE natively handles hierarchical search spaces by modeling each hyperparameter individually by two one-dimensional distributions. Evolutionary Algorithm: Genetic Programming, genetic programming should only be used for CASH in combination with ML pipeline structure search. Because some concepts cannot be easily applied to algorithm selection: a crossover between a SVM and a decision tree cannot be modeled reasonably. Particle Swarm Optimization, PSO tends to converge faster to a local minimum in comparison to genetic programming . Multi-Armed Bandit Learning, Multi-armed bandit learning is limited to a finite number of bandits. Gradient Descent, A very powerful optimization method is gradient descent, an iterative minimization algorithm. 在第四部分，论文主要介绍了一些解决CASH问题的方法。 算法选择和超参数优化两个步骤同步进行，这个简称CASH问题。 网格搜索：这是第一个系统搜索配置空间的方法。网格搜索为配置创建了一个网格，并且对它们进行了评估。同时每一个连续的超参数都被等价地离散化为k个值。这个基础的算法不能很好地解决较大配置空间的问题，因为评估函数的数量将会随着超参数的数量指数级增长。 随机搜索：通过随机地为每一个独立的超参数选择一个值，便可以产生一个候选的配置，随机搜索很容易实现，也很容易并行化，非常适合具有许多局部最小值的无梯度函数。 基于序列模型的优化： 高斯过程：尽管一个高斯过程是一个无参数的模型，但它本身依旧需要超参数，比如m和k的选择。高斯过程一个普遍的缺点就是由于k的反转而导致的高时间复杂度O(n3)。 随机森林：随机森林回归是有多个回归树组成一个集成方法 树形结构的parzen估计：TPE通过两个一维分布分别对每个超参数进行建模，进而在本地处理分层搜索空间 进化算法： 遗传编码：遗传编码只能用于CASH和机器学习流水线结构查找相结合，因为某些概念不能轻易应用于算法选择：SVM和决策树之间的交叉无法合理地建模。 粒子群优化算法：和遗传编码相比，PSO更快地收敛到局部最小值。 多臂赌博机学习：仅限于有限数量的赌博机。 梯度下降法：基于迭代的最小化算法。 Fifth SectionData cleaning is an important aspect of building an ML pipeline. The purpose of data cleaning is improving the quality of a data set by removing data errors. Common error classes are missing values in the input data, invalid values or broken links between entries of multiple data sets. 数据清理是构建ML流水线的一个重要方面。 数据清洗的目的通过消除数据错误来提高数据集的质量。 常见错误类别有输入数据中缺少的值，无效值或多个数据集的条目之间的连接缺失。 Sixth SectionThe task of feature engineering is highly domain specific and very difficult to generalize. Even for a data scientist assessing the impact of a feature is difficult, as domain knowledge is necessary. Consequently, feature engineering is a mainly manual and time-consuming task driven by trial and error. Feature selection is the easier part of feature engineering as the search space is very limited: each feature can either be included or excluded. Consequently, many different algorithms for feature selection exist. Basically all automatic feature generation approaches follow the iterative scheme. Based on an initial data set, a set of candidate features is generated and ranked. High ranking features are evaluated and potentially added to the data set. These three steps are repeated several times. 特征工程的任务是和领域高度相关的，很难统一概括。 因此即使是数据科学家也很难对特征的优劣进行评估。 因此，特征工程是一个主要由人类参与且耗时巨大的试错过程。 特征选择是特征工程中比较容易的部分，因为搜索空间非常有限：可以包含或排除某个特征。 现在存在许多用于特征选择的不同算法。 而至于特征生成方面，基本上所有自动特征生成方法都遵循迭代方案。 基于初始数据集，生成并排列一组候选特征。 评估并对这些其中排序靠前的特征，并有可能添加到数据集中。 这三个步骤重复几次，直到生成符合要求的特征。 Seventh SectionIn seventh section, this paper introduces different performance improvements. Multi-Fidelity Approximations: Depending on the used data set, fitting a single model can take several hours, in extreme cases even up to several days. Consequently, optimization progress is very slow. A common approach to circumvent this limitation is the usage of multi-fidelity approximations. By testing a configuration on this training subset, bad performing configurations can be discarded very fast and only well performing configurations have to be tested on the complete training set. Early Stopping: A quite simple approximation is aborting the fitting after the first fold if the performance is significantly worse than the current incumbent. Scalability: A common strategy for solving a computational heavy problem is parallelization on multiple cores or within a cluster. As AutoML normally has to fit many ML models, distributing different fitting instances in a cluster is an obvious idea. Ensemble Learning: Ensemble methods combine multiple ML models to create predictions. Depending on the diversity of the combined models, the overall accuracy of the predictions can be significantly increased. The cost of evaluating multiple ML models is often neglectable considering the performance improvements. Meta-Learning: Meta-learning can be used in multiple stages of automatically building an ML pipeline to increase the efficiency. Search Space Refinements, Filtering of Candidate Configurations, Warm-Starting, Pipeline Structure. 在第七部分，论文介绍了几种不同的优化方法，多保真估计，早停，扩展，集成学习和元学习。 多保真估计：通过测试该训练子集上的配置，可以非常快速地丢弃性能差的配置，并且只需要在完整的训练集上测试性能良好的配置。 早停： 一个非常简单的近似是在第一次折叠后如果性能明显比当前的差，那么中止拟合。 扩展：由于AutoML通常必须适合许多ML模型，因此在一个群集中分配不同的拟合实例是一个直接的想法。 集成学习：集成学习结合多个ML模型来进行预测。 元学习：元学习可以在自动构建ML流水线的多个阶段中使用，以提高效率。 Eighth SectionIn this section, papers introduces some existing frameworks: 在第八部分，论文介绍了一些现有的框架。 Appendix Ⅰ：This Survey Mind map]]></content>
      <categories>
        <category>Paper Notes</category>
      </categories>
      <tags>
        <tag>AutoML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习读书笔记（Ⅰ）]]></title>
    <url>%2F2019%2F07%2F24%2FMachineLearningBookNotes%2F</url>
    <content type="text"><![CDATA[Notes on Machine Learning重要名词解释Important Expressions训练集train set： 用于模型训练的数据集 验证集validation set：用于进行模型评估和模型选择的数据集 测试集test set：用来评估模型在实际使用中的泛化能力 形象上来说训练集就像是学生的课本，学生 根据课本里的内容来掌握知识，验证集就像是作业，通过作业可以知道 不同学生学习情况、进步的速度快慢，而最终的测试集就像是考试，考的题是平常都没有见过，考察学生举一反三的能力。 错误率error rate:分类错误的样本数占样本总数的比例 精度accuracy：分类正确的样本数占样本总数的比例 线性模型Linear Model线性判别分析Linear Discriminant Analysis,LDA:用于解决二分类问题的经典线性学习方法。主要思想：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离。 多分类学习的基本思路是“拆解法”，即将多分类任务拆为若干个二分类任务求解。 最经典的拆分策略有三种：“一对一”（One vs One, OvO)，“一对其余”（One vs Rest，OvR) 和 “多对多”（Many vs Many，MvM） 类别不平衡问题class-imbalance:指分类任务中不同类别的训练样例数目差别很大的情况。三类处理方案，第一类是欠采样（under sampling)即去除训练集中数目较多类别的样例；第二类是过采样（over sampling)即增加训练集中数目较少类别的样例；第三类是直接基于原始训练集进行训练，但在使用分类器进行预测时修改决策过程，进行“阈值移动”（threshold moving)。 决策树Decision Tree决策树的划分目标：决策树的分支节点所包含的样本尽可能属于同一类别，即结点的“纯度”（purity）越来越高。 神经网络Neural Networks多层前馈神经网络（multi-layer feedforward neural networks)：每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络称之为多层前馈神经网络。 神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”（connection weight）以及每个功能神经元的阈值；换言之。神经网络学到的东西，蕴含在连接权与阈值中。 常见的神经网络： RBF(Radial Basis Function，径向基函数)网络，这是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。 ART（Adaptive Resonance Theory， 自适应谐振理论）网络，它采用竞争型学习（一种常用的无监督学习策略）的策略，由比较层、识别层、识别阈值和重置模块构成。 SOM（Self-Organizing Map，自组织映射）网络，这是一种竞争学习型的无监督神经网络，它能将高位输入数据映射到低维空间（通常是二维）,同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。 级联相关网络（cascade-correlation) 是结构自适应的神经网络。 Elman网络是递归神经网络。与前馈神经网络不同，递归神经网络（recurrent neural networks)允许网络中出现环形结构，从而可以让一些神经元的输出反馈回来作为输入信号。 Boltzmann机：这是一种“基于能量的模型”（energy-based model)。神经网络中有一类模型是为网络状态定义一个“能量”（energy），能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。 深度学习deep learning：典型的深度学习模型就是很深层的神经网络。对于神经网络模型，提高容量的一个简单办法就是增加隐层数目，然而多隐层神经网络难以直接使用经典算法（比如标准BP算法）进行训练，因为误差在多隐层内逆传播时，往往会“发散”（diverge）而不能收敛到稳定状态，所以无监督逐层训练（unsupervised layer-wise training)是多隐层网络训练的有效手段，即采用预训练+微调的做法对多隐层网络进行训练，这样可以有效地节省了训练开销。 另一种节省训练开销的策略是”权共享（weight sharing)”，即让一组神经元使用相同的连接权，这个策略在卷积神经网络(Convolutional Neural Network)发挥了重要作用。 贝叶斯分类Bayes Classifier*朴素贝叶斯分类器（naive Bayes classifier) *：朴素贝叶斯分类器采用了属性条件独立假设（attribute conditional independence assumption)，即对已知类别，假设所有属性相互独立，换言之，假设每个属性独立地对分类结果发生影响。之所以使用朴素贝叶斯分类器，是因为贝叶斯分类器难以从有限的训练样本中直接估计所有属性上的联合概率。 EM(Expectation-Maximization)算法：是一种迭代式的方法，用于计算参数隐变量，其基本思想是：若参数Θ已知，则可以根据训练数据推断出最优隐变量Z的值（E步）；反之，若Z的值已知，则可以方便地对参数Θ做最大似然估计（M步）。 集成学习Ensemble Learning*集成学习(ensemble learning) *：通过构建并结合多个学习器来完成学习任务，也被称为多分类器系统（multi-classifier system)。根据个体学习器的生成方式，目前的集成学习方法大致可分为两类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，代表算法是Boosting；另一类是个体学习器之间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和随机森林（Random Forest)。 Boosting：这是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。 Bagging：这是一种并行式集成学习方法，它直接基于自助采样法（Bootstrap sampling)。给定包含m个样本的数据集，先随机取出一个样本放入采样集中，再把样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到m个样本的采样集。其中初始训练集中约有63.2%的样本出现在采样集中。照这样可以采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合，这就是Bagging的基本流程。 随机森林(Random Forest，RF)：RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统的决策树在选择划分属性时实在当前结点的属性集合（假设有d个属性）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最有属性用于划分，这里的参数k控制了随机性的引入程度。 结合策略：平均法，投票法和学习法（通过另一个学习器来结合个体学习器，即通过次级学习器来结合初级学习器）。 聚类Clustering聚类(clustering)：聚类算法是无监督学习（unsupervised learning)，训练样本的标记信息未知，通过聚类，试图将数据集中的样本划分为若干个通常不相交的子集，每个子集称为一个“簇”（cluster)。聚类过程仅能自动形成簇结构，而簇所对应的概念语义需要使用者来进行命名和把握。聚类算法涉及的两个基本问题是性能度量和距离计算。 性能度量(validity index)：性能度量大致有两类： 一类是将聚类结果与某个参考模型（reference model)比较，称为“外部指标”（external index），常用的外部指标有Jaccard系数（Jaccard Coefficient, JC），FM指数（Fowlkes and Mallows Index, FMI)和Rand指数（Rand Index,RI)。这些指数越大说明聚类效果越好。 另一类是直接考察聚类结果而不利用任何参考模型，称为“内部指标”（Internal index)，常用的内部指标有DB指数（Davies-Bouldin Index, DBI）和Dunn指数（Dunn Index,DI)。内部指标中DBI的值越小越好，而DI的值越大越好。 密度聚类（density-based clustering）：通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果。DBSCAN是一种著名的密度聚类算法。 层次聚类（hierarchical clustering）：这种聚类算法试图在不同层次对数据集进行划分，从而形成树形的聚类结构。AGNES是一种采用自底向上聚合策略的层次聚类算法。 半监督学习Semi-supervised Learning半监督学习（semi-supervised learning)：让学习器不依赖外界交互，自动利用未标记样本来提升学习性能。 半监督支持向量机（semi-supervised support vector machine, S3VM)：这是支持向量机在半监督学习上的推广。 半监督聚类（semi-supervised clustering)：聚类是典型的无监督学习任务，但现实的聚类任务中，我们往往能获取到一些额外的监督信息，这些信息大致有两类，一类是必连（指样本必属于同一个簇）与勿连（指样本必不属于同一个簇）约束，另一类是少量的标记样本。利用这些监督信息，我们可以通过半监督聚类来获得更好的聚类效果。 规则学习Rule Learning规则学习（rule learning)：规则学习是从训练数据中学习出一组能用于对未见示例进行判别的规则。 序贯覆盖（sequential covering)：即逐条归纳，在训练集上每学到一条规则，就将该规则覆盖的训练样例去除，然后以剩下的训练样例组成训练集重复上述过程。由于每次只处理一部分数据，因此也被称为“分治”（separate-and-conquer)策略。 一阶规则学习，FOIL算法（First-Order Inductive Learner)：FOIL是著名的一阶规则学习算法，它遵循序贯覆盖框架并且采用自顶向下的规则归纳策略，由于逻辑变量的存在，FOIL在规则生成时需要考虑不同的变量组合。 归纳逻辑程序设计（Inductive Logic Programming，ILP）：归纳逻辑程序设计在一阶规则学习中引入了函数和逻辑表达式嵌套。 强化学习Reinforcement Learning强化学习：强化学习的学习目的就是要找到能使长期累积奖赏最大化的策略。强化学习中没有标记样本，即没有人直接告诉机器在什么状态下应该做什么动作，只有等到最终结果揭晓，才能通过“反思”之前动作是否正确来进行学习，因此，强化学习在某种意义上可看作具有“延迟标记信息”的监督学习问题。 探索与利用(Exploration and Exploitation)：探索（“估计摇臂的优劣”）和利用（“选择当前最优摇臂”）这两者是矛盾的，因为尝试次数有限，加强了一方则会自然削弱另一方，这是强化学习所面临的“探索利用窘境”（Exploration-Exploitation dilemma)。显然，想要强化学习累积奖赏最大，需要在探索和利用之间达到较好的折中。 模仿学习（imitation learning)：在强化学习的经典任务设置中，机器所能获得的反馈信息仅有多步决策后的累计奖赏，但在现实任务中，往往能得到人类专家的决策过程范例，从这样的范例中学习，称为“模仿学习”（imitation learning）。]]></content>
      <categories>
        <category>Book Notes</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习读书笔记（Ⅰ）]]></title>
    <url>%2F2019%2F07%2F24%2FDeepLearningBookNotes%2F</url>
    <content type="text"><![CDATA[Deep Learning Book Notes深度前馈网络deep feedforward network基于梯度的学习——代价函数： 使用最大似然学习条件分布。大多数现代的神经网络使用最大似然来训练。这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价。 均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不佳，一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度，这就是为什么交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一。 基于梯度的学习——输出单元： 代价函数的选择与输出单元的选择密切相关。任何可用作输出的神经网络单元，也可以被用作隐藏单元。 用于高斯输出分布的线性单元：一种基于仿射变换的输出单元，仿射变换不具有非线性，所以这些单元往往直接被称为线性单元。 用于Bernoulli输出分布（伯努利分布）的sigmoid单元：许多任务需要预测而执行变量的值。具有两个类的分类问题可以归结为这种形式。 用于Multinoulli输出分布（多项分布）的softmax单元：任何时候当我们想要表示一个具有n个可能取值的离散型随机变量的分布时，我们都可以使用softmax函数。它可以看作是sigmoid函数的扩展，其中sigmoid函数用来表示二值型变量的分布。 隐藏单元： 整流线性单元及其扩展：整流线性单元易于优化，它与线性单元非常类似。线性单元和整流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为零。 logistic sigmoid与双曲正切函数： 其他隐藏单元：softmax单元，径向基函数（rational basis function，RBF），softplus函数，硬双曲正切函数（hard tanh) 深度学习中的正则化和优化深度学习中的正则化：对学习算法进行修改，旨在减少泛化误差而不是训练误差。模型族训练有三个情形： 不包括真实的数据生成过程，对应欠拟合和含有偏差的情况； 匹配真实数据生成过程； 除了包括真实的数据生成过程，还包括许多其他可能的生成过程——方差（而不是偏差）主导的过拟合。 正则化的目标是使模型从第三种情况转化为第二种情况。 深度学习中的优化：用于深度模型训练的优化算法与传统的优化算法在几个方面有所不同。机器学习通常是间接作用的。在大多数机器学习问题中，我们关注某些性能度量P，其定义于测试集上并且可能是不可解的。因此，我们只是间接地优化P。我们希望通过降低代价函数J(θ)来提高P，这一点与纯优化不同，纯优化最小化目标J本身。训练深度模型的优化算法通常也会包括一些针对机器学习目标函数的特定结构进行优化。 卷积神经网络convolutional neural network卷积神经网络，也叫做卷积网络，指那些至少在网络的一层中使用卷积运算（卷积运算是一种特殊的线性运算）来替代一般矩阵乘法运算的神经网络。这种网络是一种专门用来处理具有类似网格结构的数据的神经网络，比如时间序列数据和图像数据。 卷积运算： s(t) = ∫x(a)w(t - a)da 上面这种运算称为卷积（convolution）,卷积运算通常用星号*表示。 s(t) = (x * w)(t) 在卷积运算的术语中，卷积的第一个参数（上式中的x）通常叫做输入（input)，第二个参数（上式中的函数w）叫做核函数（kernel），输出有时被称为特征映射（feature map)。 动机：卷积运算通过三个重要思想来帮助改进机器学习系统：稀疏交互（sparse interactions)、参数共享（parameter sharing）、等变表示（equivariant representations)。 池化：卷积网络中一个典型层包含三级。第一级中，这一层并行地计算多个卷积产生一组线性激活响应。第二级中，每一个线性激活响应将会通过一个非线性的激活函数，例如整流线性激活函数。这一级有时也被称为探测级。第三级中，我i们使用池化函数来进一步调整这一层的输出。 池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。 循环神经网络recurrent neural network循环神经网络rnn：这是一类用于处理序列数据的神经网络。 循环神经网络中的一些重要的设计模式： 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络。 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络。 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络。 长短期记忆LSTM：引入自循环的巧妙构思，以产生梯度长时间持续流动的路径。]]></content>
      <categories>
        <category>Book Notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
</search>
