<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[NLP论文阅读笔记（Ⅱ）]]></title>
    <url>%2F2019%2F09%2F02%2FNLPpaperNotes2%2F</url>
    <content type="text"><![CDATA[Attention is all you need Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 2017-Decem(Nips), 5999–6009. Retrieved from https://arxiv.org/pdf/1706.03762.pdf Introduction Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 递归模型通常沿着输入和输出序列的符号位置进行因子计算。这个固有的顺序排除了使用并行化的可能，而并行化在长序列的训练中非常关键，因为内存约束限制批处理。 在各种任务中，注意力机制已经成为引人注目的序列建模和转换模型的一个组成部分，允许对依赖项进行建模，而不考虑它们在输入或输出序列中的距离。然而，这种注意力机制经常与递归模型一起使用。 这篇论文提出了一种Transformer，这是一种模型架构，它没有采用递归模型，而是完全依赖一种注意力机制来生成输入和输出之间的全局依赖。这种Transformer允许并行化，并且经过训练之后能够在翻译质量方面达到一个新的高度。 Model Architecture The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization. The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The two most commonly used attention functions are additive attention , and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. Transformer 遵循图一的总体架构，使用堆叠的自我注意机制，以及针对编码器和解码器的全连接层。 这个架构中的编码器是由N = 6个相同的层组成。每一层有两个子层，第一个是一个多头的自我注意机制，第二个是一个简单的全连接的前馈网络，另外在每一个子层周围使用一个剩余连接并进行层标准化。 类似的，解码器也由N = 6个相同的层组成。除每个编码器层中的两个子层外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意。 注意函数可以说是将查询和一组键值对到输出的映射，其中查询、键、值和输出都是向量。两个最常用的注意函数是加法注意和点积(乘法)注意。点积注意与我们的算法相同。加性注意是利用带有单隐层的前馈网络计算兼容性函数。虽然这两种方法在理论上的复杂度相似，但由于点积可以使用高度优化的矩阵乘法代码来实现，因此点积注意在实践中更快、更节省空间。 这Transformer架构中，有三个地方使用自我注意机制。第一，在“编解码器注意”层中，查询来自于前一解码器层，存储键值来自于编码器的输出。第二，编码器包含自我注意层。在一个self-attention层中，所有的键值和查询都来自同一个位置，在本例中，这个位置是编码器中上一层的输出。第三，解码器中的自我注意层允许解码器中的每个位置关注解码器中之前层的所有位置。 Why Self-Attention we compare various aspects of self-attention layers to the recurrent and convolutional layers. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. As side benefit, self-attention could yield more interpretable models. 这篇论文在这一部分将自我注意层的各个方面与递归层和卷积层进行了比较。最终考虑到以下三个因素，选择了自我注意机制。一个是每层的总计算复杂度。另一个是可以并行化的计算量，由所需的最小顺序操作数来衡量。第三个是网络中远程依赖项之间的路径长度。另外，作为附带好处，自我注意机制可以产生更多可解释的模型。 Training and Results On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. 在2014年WMT英德翻译任务中，big transformer model(见表2中的transformer (big))的性能比之前报道的最佳模型(包括集成电路)高出2.0 BLEU以上，建立了一个新的最先进的BLEU评分28.4。 为了评估transformer 不同组件的重要性，这篇论文以不同的方式改变了基本模型，在开发集newstest2013上测量了英语到德语翻译的性能变化。 表4中的结果显示，尽管缺少特定于任务的调优，但是模型执行得出奇地好，除了递归神经网络语法之外，它比所有先前报告的模型都产生了更好的结果。 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. 这篇论文提出了一个Transformer，这是第一个完全基于注意机制的序列转换模型，使用多头自注意机制取代了编码器和解码器架构中最常用的循环层，最终在机器翻译这方面也能得到一个更好的效果，并且还具有较好的拓展性，未来可能在其他任务上表现出优良性能。 Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., &amp; Salakhutdinov, R. (2019). Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. Retrieved from http://arxiv.org/abs/1901.02860 http://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html Introduction We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length with- out disrupting temporal coherence. Our main technical contributions include introducing the notion of recurrence in a purely self- attentive model and deriving a novel positional encoding scheme. These two techniques form a complete set of solutions, as any one of them alone does not address the issue of fixed-length contexts. Transformer-XL is the first self-attention model that achieves substantially better results than RNNs on both character-level and word-level language modeling. 长期依赖关系问题是序列模型中常见的现象，使用神经网络来解决长期依赖关系仍具有挑战性，例如基于Gating的RNN和梯度裁剪技术（gradient clipping）虽然具有一定的解决长期依赖关系的能力，但还不能完全解决这个问题，而Transformer可以很好的获取长期依赖关系，但是仅限于固定长度的上下文，即将一个长的文本序列截断为几百个字符的固定长度片段，然后分别处理每个片段。这就产生了两个关键的限制，依赖关系的长度受限以及上下文碎片化。 而这篇论文为了解决这些限制，提出了一个新的架构，Transformer-XL(meaning extra long)，它能够使得依赖关系不受固定长度的限制，而且还有一些新的优势。Transformer-XL 主要有两种技术组成，一种是片段级递归机制(segment-level recurrence mechanism)，另一种是相对位置编码方案(relative positional encoding scheme) Model To address the limitations of using a fixed-length context, we propose to introduce a recurrence mechanism to the Transformer architecture. During training, the hidden state sequence computed for the previous segment is fixed and cached to be reused as an extended context when the model processes the next new segment. Besides achieving extra long context and resolving fragmentation, another benefit that comes with the recurrence scheme is significantly faster evaluation. Specifically, during evaluation, the representations from the previous segments can be reused instead of being computed from scratch as in the case of the vanilla model. In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Under the new parameterization, each term has an intuitive meaning: term (a) represents content- based addressing, term (b) captures a content- dependent positional bias, term (c) governs a global content bias, and (d) encodes a global positional bias. 片段级递归机制: 训练过程中，对前一个分段计算的表示进行修复并缓存，以便在模型处理下一个新的分段时作为扩展上下文重新利用。这种额外的连接将最大可能依赖关系长度增加了N倍，其中N表示网络的深度，因为上下文信息现在可以跨片段边界流动。此外，这种递归机制还解决了上下文碎片问题，为新段前面的token提供了必要的上下文。 然而，单纯应用片段级递归机制是不行的，因为当重用前面的段时，位置编码是不一致的。因此，这篇论文提出了一种新的相对位置编码方案，使递归机制成为可能。 Experiments We apply Transformer-XL to a variety of datasets on both word-level and character-level language modeling to have a comparison with state-of-the- art systems, includingWikiText-103 (Merity et al., 2016), enwik8 (LLC, 2009), text8 (LLC, 2009), One Billion Word (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012). We conduct two sets of ablation studies to examine the effects of two proposed techniques used in Transformer-XL: the recurrence mechanism and the new positional encoding scheme. Trained only on WikiText-103 which is medium- sized, Transformer-XL is already able to generate relatively coherent articles with thousands of to- kens without manual cherry picking, despite minor flaws. Finally, we compare the evaluation speed of our model with the vanilla Transformer model (Al- Rfou et al., 2018). As shown in Table 9, due to the state reuse scheme, Transformer-XL achieves an up to 1,874 times speedup during evaluation. Transformer-XL在各种主要的语言建模(LM)基准测试中获得新的最优(SoTA)结果，包括长序列和短序列上的字符级和单词级任务。实验证明， Transformer-XL 有三个优势： Transformer-XL学习的依赖关系比RNN长约80%，比vanilla Transformers模型长450%。（尽管后者在性能上比RNN好，但由于固定长度上下文的限制，对于建模长期依赖关系并不是最好的。） 因为不需要重复计算，Transformer-XL在语言建模任务的评估期间比vanilla Transformer快1800+倍。 因为建模长期依赖关系的能力，Transformer-XL在长序列上具有更好的Perplexity, 预测样本方面更准确，并且通过解决上下文碎片化问题，在短序列上也具有更好的性能。 Conclusions Transformer-XL obtains strong perplexity results, models longer-term dependency than RNNs and Transformer, achieves substantial speedup during evaluation, and is able to generate coherent text articles. Transformer XL比RNN和Transformer具有较强的perplexity，对长期依赖关系进行建模，在评估过程中实现了实质性的加速，能够生成连贯的文本文章。]]></content>
      <categories>
        <category>Paper Notes</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP论文阅读笔记（Ⅰ）]]></title>
    <url>%2F2019%2F08%2F27%2FNLPpaperNotes%2F</url>
    <content type="text"><![CDATA[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (Mlm). Retrieved from http://arxiv.org/abs/1810.04805 Introduction There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre- trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective. Unlike left-to- right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. 这篇论文首先总结了现存的两种将预训练语言表示用于下游任务的方法，分别是基于特征的方法和微调方法。基于特征的方法比如ELMo，使用包含预训练表示的特定任务架构作为额外的特征。而微调方法比如OpenAI GPT，引入了最少的特定任务的参数，通过简单地微调所有预训练参数就可以完成在下游任务的训练。这两种方法在预训练过程中使用了同样的目标函数，它们都使用了单向的语言模型来学习通用的语言表示。但这篇论文认为使用单向语言模型会限制预训练表示的能力，尤其是在微调方法中，主要是限制了在预训练中能够被使用架构的选择。 在这篇论文中，作者改进了微调方法，提出了BERT，来自转换器的双向编码表示。BERT减少了单向模型对预训练模型的限制，主要是通过使用一个MLM预训练目标函数。不像从左到右的单向预训练模型，MLM目标函数能够让语言表示结合左右的文本，最终能够生成一个深度的双向转换器。 Related Work ELMo and its predecessor generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. There has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference and machine translation. 这篇论文简单介绍了一些广泛使用的预训练语言表示方法。主要分为三类，分别是无监督的基于特征的方法，无监督的微调方法，以及从监督数据中迁移学习。 无监督的基于特征的方法中最具代表性的是ELMo方法，这种方法将传统的词向量研究从一个不同的维度进行了一般化。这种方法从从左到右和从右到左的语言模型中提取出了上下文敏感的特征。无监督的微调方法中表现良好的是OpenAI GPT，这种方法的一个优势就是需要学习的参数比较少。而关于迁移学习，主要就是从一些基于大数据集的监督学习任务（比如自然语言推断和机器翻译）中进行有效的迁移。 BERT There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine- tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section. A distinctive feature of BERT is its unified architecture across different tasks. BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together. We pre-train BERT using two unsupervised tasks. One task is Masked LM, In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature . The other task is Next Sentence Prediction (NSP). In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Fine-tuning is straightforward since the self- attention mechanism in the Transformer allows BERT to model many downstream tasks— whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs. 在BERT的框架中有两个步骤: 预训练和微调。在预训练中，使用不同训练任务的未标记数据进行训练。至于微调，首先使用预先训练的参数初始化BERT模型，然后使用来自下游任务的标记数据对所有参数进行微调。每个下游任务都有单独的微调模型，即使它们是用相同的预训练参数初始化的。图1中的问答示例大致表示了BERT的框架。 BERT的一个显著特征是它针对不同任务有统一架构。BERT的模型架构是一个多层双向转换器的编码器。BERT的输入序列可以是一个句子或两个组合在一起的句子。 我们使用两个非监督任务对BERT进行预训练。其中一个任务是Masked LM, 为了训练一个深层的双向表示，我们只需随机掩码输入符号的比例，然后预测这些掩码符号。我们将这个过程称为“Masked LM”(MLM)。另一项任务是下一句预测(NSP)。为了训练一个理解句子关系的模型，我们对一个可以由任何单语语料库生成的二值化下一句预测任务进行了预训练。 微调是相对简单的，因为转换器的自我注意机制允许BERT通过交换适当的输入和输出来为许多下游任务建模——无论是单个文本还是文本对。 Experiments We present BERT fine-tuning results on some NLP tasks. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a collection of diverse natural language understanding tasks. Both BERTBASE and BERTLARGE outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowd- sourced question/answer pairs. Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensemble and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble system in terms of F1 score. The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided para- graph, making the problem more realistic. We observe a +5.1 F1 improvement over the previous best system. The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded common- sense inference. BERTLARGE out- performs the authors’ baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%. 这篇论文在几个NLP任务上进行了实验，并给出了Bert微调方法的结果。在通用语言理解评估（GLUE），斯坦福问答数据集（sQuAD 1.1 ，2.0）和常识推理对抗数据集（SWAG）上面分别进行了实验，Bert在上面表现效果良好，甚至远远优于现有的最好方法。 Ablation Studies We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pre- training objectives using exactly the same pre- training data, fine-tuning scheme, and hyperparameters as BERTBASE. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously. We compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task. BERT is effective for both fine- tuning and feature-based approaches. 这篇论文为了说明Bert的重要性，进行了一系列对比试验（即Ablation Study)。这些实验分别说明了预训练任务的影响，模型大小的影响，以及通过基于特征的Bert方法的实验说明了BERT对于微调和基于特征的方法都很有效。 Conclusion Our major contribution is further generalizing some existing findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks. 这篇论文的主要贡献是进一步将一些现有的发现推广到深层双向架构，能够使用相同的预训练模型成功地处理广泛的NLP任务。 Deep contextualized word representations Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). Deep Contextualized Word Representations. 2227–2237. https://doi.org/10.18653/v1/n18-1202 Introduction We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations. ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM. More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer. Extensive experiments demonstrate that ELMo representations work extremely well in practice. 这篇论文使用了一个双向LSTM向量，这个向量在大型文本语料库中由一个耦合的语言模型（LM）目标训练而来，因此，本文称它是ELMo表示(语言模型的嵌入)，ELMo表示是深层次的，因为它是biLM所有内层的函数。具体来说，ELMo学习了一个向量的线性组合，这些向量堆叠在每个输入词之上，为每个结束任务使用，这样相比于仅仅使用顶层的LSTM层，显著地改善了性能。这篇论文给出了一些实验来证明ELMo表示在实践中表现效果良好。 Related Work In this paper, we take full advantage of access to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences. We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks. In contrast, after pretraining the biLM with unlabeled data, we fix the weights and add additional task- specific model capacity, allowing us to leverage large, rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model. 这篇论文充分利用了对大量单语数据的访问，并在一个约有3000万个句子的语料库上训练biLM。这篇论文还将这些方法推广到深层上下文表示，这些方法可以很好地处理各种NLP任务。此外，在使用未标记的数据对biLM进行预训练之后，这篇论文确定了权重，并添加了额外的特定于任务的模型容量，从而能够利用大型、丰富和通用的biLM表示来处理下游训练数据大小要求较小的监督模型的情况。 ELMo: Embeddings from Language Models Unlike most widely used word embedding, ELMo word representations are functions of the entire input sentence. They are computed on top of two-layer biLMs with character convolutions , as a linear function of the internal network states. This setup allows us to do semi-supervised learning, where the biLM is pre- trained at a large scale and easily incorporated into a wide range of existing neural NLP architectures. A biLM combines both a forward and backward LM. Our formulation jointly maximizes the log likelihood of the forward and backward directions. We share some weights between directions instead of using completely independent parameters. ELMo is a task specific combination of the intermediate layer representations in the biLM. To add ELMo to the supervised model, we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo(task k) with x(k)and pass the ELMo enhanced representation [x(k); ELMo(task k) ] into the task RNN. Finally, we found it beneficial to add a moderate amount of dropout to ELMo. Once pretrained, the biLM can compute representations for any task. In some cases, fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance. 与最广泛使用的单词嵌入不同，ELMo单词表示是整个输入语句的函数。它们是在带有字符卷积的两层biLMs上计算的，并作为内部网络状态的线性函数。这种设置允许进行半监督学习，其中biLM是预先在大规模数据中训练的，并很容易纳入现有的神经NLP架构的广泛范围。 biLM结合了正向LM和反向LM。这篇论文的公式最大化了正向和反向的对数可能性，在方向之间共享一些权重，而不是使用完全独立的参数。 ELMo是biLM中中间层表示形式的特定于任务的组合。为了将ELMo添加到监督模型中，首先冻结biLM的权值，然后将ELMo向量ELMo(task k)与x(k)连接起来，将ELMo的增强表示[x(k);ELMo(task k)传递到任务RNN中。最后发现在ELMo中添加适量的dropout是能够提升性能的。 经过预训练后，biLM可以为任何任务计算表示形式。在某些情况下，对领域特定数据的biLM进行微调会显著降低复杂性，并提高下游任务性能。 Evaluation Table 1 shows the performance of ELMo across a diverse set of six benchmark NLP tasks. In every task considered, simply adding ELMo establishes a new state-of-the-art result, with relative error reductions ranging from 6 - 20% over strong base models. This is a very general result across a diverse set model architectures and language understanding tasks. 表1显示了ELMo在6个基准NLP任务中的性能。在考虑的每一个任务中，简单地添加ELMo就可以建立一个新的最先进的结果，相对于强基础模型，误差降低了6 - 20%。这是一个非常普遍的结果，适用于不同的集合模型体系结构和语言理解任务。 Analysis This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations. Using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer. Additionally, we analyze the sensitivity to where ELMo is included in the task model, training set size, and visualize the ELMo learned weights across the tasks. 这篇论文在这部分进行了控制变量分析，以验证论文的主要观点，并介绍了一些ELMo表示有趣的方面。首先，验证了在下游任务中使用深层上下文表示比只使用顶层的先前工作提高了性能。此外，这部分还分析了ELMo在任务模型中的位置、训练集大小的敏感性，并可视化了ELMo在任务中学习到的权重。 Conclusion We have introduced a general approach for learning high-quality deep context-dependent representations from biLMs, and shown large improvements when applying ELMo to a broad range of NLP tasks. the biLM layers efficiently encode different types of syntactic and semantic information about words- in-context, and using all layers improves overall task performance. 我们介绍了一种从biLMs学习的高质量深度上下文相关表示的通用方法ELMo，并将ELMo应用于广泛的NLP任务,相比之前的方法有很大改进。此外，biLM层可以有效地编码关于上下文中的单词的不同类型的语法和语义信息，并且使用所有层可以提高整体任务性能。 Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction Alt, C., Hübner, M., &amp; Hennig, L. (2019). Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction. Retrieved from http://arxiv.org/abs/1906.08646 Introduction Relation extraction (RE), defined as the task of identifying the relationship between concepts mentioned in text, is a key component of many natural language processing applications. Current state-of-the-art RE methods try to address these challenges by applying multi-instance learning methods and guiding the model by explicitly provided semantic and syntactic knowledge. However, we observe that these models are often biased towards recognizing a limited set of relations with high precision, while ignoring those in the long tail. In this paper, we introduce a Distantly Supervised Transformer for Relation Extraction (DISTRE). We extend the standard Transformer architecture by a selective attention mechanism to handle multi-instance learning and prediction, which allows us to fine-tune the pre-trained Transformer language model directly on the distantly supervised RE task. This minimizes explicit feature extraction and reduces the risk of error accumulation. We selected the GPT as our language model because of its fine-tuning efficiency and reasonable hardware requirements, compared to e.g. LSTM- based language models or BERT. 关系提取(RE)是许多自然语言处理应用程序的关键组成部分，它是指识别文本中提到的概念之间的关系。目前最先进的RE方法试图通过应用多实例学习方法来解决关系提取中遇到的问题，并通过显式提供语义和语法知识来指导模型。然而，这些方法往往倾向于识别一组有限的高精度关系，而忽略了长远的关系。 这篇论文介绍了一种用于关系提取的远距离监督Transformer。通过选择性注意机制扩展了标准的Transformer体系结构，以处理多实例学习和预测，这样能够在远距离监督任务上直接微调预训练的Transformer语言模型。这使得显式特征提取最小化，并降低了错误积累的风险。这篇论文选择GPT作为语言模型，是因为和基于LSTM的语言模型或BERT相比，它的微调效率较高以及硬件需求合理。 Transformer Language Model The Transformer-Decoder (Liu et al., 2018a), shown in Figure 2, is a decoder-only variant of the original Transformer (Vaswani et al., 2017). Like the original Transformer, the model repeatedly encodes the given input representations over multiple layers (i.e., Transformer blocks), consisting of masked multi-head self-attention followed by a position-wise feedforward operation. In contrast to the original decoder blocks this version contains no form of unmasked self-attention since there are no encoder blocks. Transformer译码器(Liu et al.， 2018a)，如图2所示，是原Transformer(Vaswani et al.， 2017)的纯译码器变体。与原始的转换器一样，该模型在多个层重复编码给出的输入表示，包括掩蔽的多头自注意机制，然后是一个位置明确的前馈操作。与原始解码器块相比，它不包含任何形式的非屏蔽自我注意，因为没有编码器块。 Multi-Instance Learning with the Transformer Our input representation (see Figure 3) encodes each sentence as a sequence of tokens. To make use of sub-word information, we tokenize the in- put text using byte pair encoding (BPE) (Sennrich et al., 2016). The BPE algorithm creates a vocabulary of sub-word tokens, starting with single characters. Then, the algorithm iteratively merges the most frequently co-occurring tokens into a new token until a predefined vocabulary size is reached. For each token, we obtain its input representation by summing over the corresponding token embed- ding and positional embedding 这篇论文的输入表示(参见图3)将每个句子编码为标记序列。为了利用子单词信息，使用字节对编码(BPE)对输入文本进行标记(Sennrich et al.， 2016)。BPE算法创建子单词标记的词汇表，以单个字符开始，然后，该算法迭代地将最频繁同时出现的标记合并到一个新的token中，直到达到预定义的词汇表大小。对于每个token，通过对相应的token嵌入和位置嵌入求和，得到其输入表示形式。 Experiment Setup We run our experiments on the distantly supervised NYT10 dataset and use PCNN+ATTN (Lin et al., 2016) and RESIDE (Vashishth et al., 2018) as the state-of-the- art baselines. The piecewise convolutional neural network (PCNN) segments each input sentence into parts to the left, middle, and right of the entity pair, followed by convolutional encoding and selective attention to inform the relation classifier with a bag- level representation. RESIDE, on the other hand, uses a bidirectional gated recurrent unit (GRU) to encode the input sentence, followed by a graph convolutional neural network (GCN) to encode the explicitly provided dependency parse tree information. This is then combined with named entity type information to obtain a sentence representation that can be aggregated via selective attention and forwarded to the relation classifier. The NYT10 dataset by Riedel et al. (2010) is a standard benchmark for distantly supervised relation extraction. Since pre-training is computationally expensive, and our main goal is to show its effectiveness by fine-tuning on the distantly supervised relation extraction task, we reuse the language model published by Radford et al. (2018) for our experiments. During our experiments we found the hyperparameters for fine-tuning, reported in Radford et al. (2018), to be very effective. 这篇论文在远程监督NYT10数据集上运行实验，使用PCNN+ATTN (Lin et al.， 2016)和RESIDE (Vashishth et al.， 2018)作为最先进的基准方法。 分段卷积神经网络(PCNN)将每个输入语句分割成实体对的左、中、右三个部分，然后进行卷积编码和选择性注意，以袋式表示的形式通知关系分类器。另一方面，RESIDE使用双向门控循环单元(GRU)对输入语句进行编码，然后使用图卷积神经网络(GCN)对显式提供的依赖解析树信息进行编码。之后将其与命名实体类型信息相结合，获得一个句子表示形式，这个表示形式可以通过选择性注意进行聚合并转发给关系分类器。 NYT10数据集是用于远程监督关系提取的标准数据集。由于预训练的计算开销很大，而主要目标是通过对远程监督关系提取任务进行微调来显示其有效性，因此这篇论文在实验中重用了Radford等人(2018)发表的语言模型。在实验中，这篇论文发现Radford等人(2018)报道的微调超参数非常有效。 Results Table 1 shows the results of our model on the held-out dataset. DISTRE with selective attention achieves a new state-of-the-art AUC value of 0.422. The precision-recall curve in Figure 4 shows that it outperforms RESIDE and PCNN+ATT at higher recall levels, while precision is lower for top predicted relation instances. 表1显示了模型在helout数据集上的结果,具有选择性注意的DISTRE达到了最新的AUC值0.422。图4中的precision-recall曲线显示，在较高的recall级别上，它的性能优于RESIDE 和PCNN+ATT，而对于顶级预测关系实例，它的精度较低。 Conclusion We proposed DISTRE, a Transformer which we extended with an attentive selection mechanism for the multi-instance learning scenario, common in distantly supervised relation extraction. While DISTRE achieves a lower precision for the 300 top ranked predictions, we observe a state-of-the-art AUC (Area Under Curve) and an overall more balanced performance. 这篇论文提出了一种用于多实例学习场景的Transformer ，它是一种常用的远程监督关系提取方法。虽然对于300个排名靠前的预测，新版本的精度较低，但它有最先进的AUC和整体更平衡的性能。]]></content>
      <categories>
        <category>Paper Notes</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NAS论文阅读笔记]]></title>
    <url>%2F2019%2F08%2F19%2FNASpaperNotes%2F</url>
    <content type="text"><![CDATA[Neural Architecture Search with Reinforcement Learning Zoph, B., &amp; Le, Q. V. (2016). Neural Architecture Search with Reinforcement Learning. 1–16. Retrieved from http://arxiv.org/abs/1611.01578 Introduction Although Neural Architecture Application has become easier, designing architectures still requires a lot of expert knowledge and takes ample time. This paper presents Neural Architecture Search, a gradient-based method for finding good architectures (see Figure 1) . It is based on the observation that the structure and connectivity of a neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network – the controller – to generate such string. Training the network specified by the string – the “child network” – on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time. 神经网络架构的应用虽然比较广泛，但是设计神经网络架构依旧需要花费很多资源和时间。鉴于此，这篇论文提出了一个基于梯度算法的方法来搜索出最优的神经网络架构，这种方法也基于一个事实——神经网络的结构和连接通常能够被一个变长的字符串指定。因此，这篇论文提出了一个循环网络，这个循环网络包含一个控制器和一个“子网络”，控制器用于产生这样的变长字符串，之后根据这个字符串训练出相应的“子网络”, 并在验证集上进行验证，产生一个准确度，然后把这个准确度作为一个反馈信号，计算出策略梯度来更新控制器，因此，在下一次迭代中，控制器将会有更高概率产生高准确率的架构，换句话说，控制器将慢慢地优化对神经网络的搜索。 Related Work Hyperparameter optimization is an important research topic in machine learning, and is widely used in practice. Despite their success, these methods are still limited in that they only search models from a fixed-length space. There are Bayesian optimization methods that allow to search non fixed length architectures (Bergstra et al., 2013; Mendoza et al., 2016), but they are less general and less flexible than the method proposed in this paper.Modern neuro-evolution algorithms，are much more flexible for composing novel models, yet they are usually less practical at a large scale. The controller in Neural Architecture Search is auto-regressive, which means it predicts hyperparameters one a time, conditioned on previous predictions. This idea is borrowed from the decoder in end-to-end sequence to sequence learning (Sutskever et al., 2014). Unlike sequence to sequence learning, our method optimizes a non-differentiable metric, which is the accuracy of the child network. It is therefore similar to the work on BLEU optimization in Neural Machine Translation (Ran- zato et al., 2015; Shen et al., 2016). Unlike these approaches, our method learns directly from the reward signal without any supervised bootstrapping. 超参数调优是在机器学习里是一个重要的研究课题，有好多方法都能得到好的效果，但是他们均只能在固定长度神经网络的搜索空间里搜索模型，另外，贝叶斯优化的方法虽然能够让这些方法搜索不定长的架构，但是这样的方法没有这篇论文中的方法更灵活。 一些现代的神经网络进化算法，在生成模型时更为灵活，但是它们通常不能用于大规模的计算。 在本文神经网络搜索里的控制器是自动回归的，也就是说它能基于之前的预测更新超参数。这个思想类似于端到端的序列学习里的解码器，但本文与之不同的是，本文的方法优化了一个不可微分的度量，这个度量是子网络的准确度。而这个度量的思想又类似于BLEU在神经网络机器翻译上的优化，但与这些方法不同的是，本文的方法直接按照反馈信息优化，而不需要监督指令。 Methods Generate Model Descriptions with A Controller Recurrent Neural Network(通过控制器循环网络产生模型) The process of generating an architecture stops if the number of layers exceeds a certain value. This value follows a schedule where we increase it as training progresses. Once the controller RNN finishes generating an architecture, a neural network with this architecture is built and trained. At convergence, the accuracy of the network on a held-out validation set is recorded. The parameters of the controller RNN, θc, are then optimized in order to maximize the expected validation accuracy of the proposed architectures. 如果网络层的数量超过了一个确定的值，那么产生神经网络架构的过程将会停止，其中这个确定的值是随着训练进程不断增加的。一旦循环网络控制器停止产生神经网络架构，那么，基于这个架构的神经网络则会被构建和训练，当神经网络趋于收敛时，这个神经网络在被留出的验证集上的准确度将会被记录。接着循环网络控制器的参数θc 将会被优化，以便产生最大期望验证集准确率的神经架构。 Training with Reinforce(利用强化学习进行训练) Use this accuracy R as the reward signal and use reinforcement learning to train the controller. More concretely, to find the optimal architecture, we ask our controller to maximize its expected reward. Since the reward signal R is non-differentiable, we need to use a policy gradient method to iteratively update θc. As training a child network can take hours, we use distributed training and asynchronous parameter updates in order to speed up the learning process of the controller . We use a parameter-server scheme where we have a parameter server of S shards, that store the shared parameters for K controller replicas. Each controller replica samples m different child architectures that are trained in parallel. The controller then collects gradients according to the results of that minibatch of m architectures at convergence and sends them to the parameter server in order to update the weights across all controller replicas. In our implementation, convergence of each child network is reached when its training exceeds a certain number of epochs. This scheme of parallelism is summarized in Figure 3 这篇论文使用模型准确度R作为反馈信号，并使用强化学习来训练控制器，具体来讲，为了找到最优的架构，控制器需要产生一个有最大准确度的模型架构。但是因为反馈信号R是不可微分的，所以这篇文章使用策略梯度方法来迭代更新控制器参数θc。 因为训练一个子网络会耗费大量时间，这篇论文采用了分布式训练和异步参数更新的方法来加快控制器的训练进程。这个方法具体来讲就是，使用一组s个参数服务器分片，这组参数服务器包含着k个控制器副本的参数，并且s个参数服务器向这k个控制器副本提供参数。而每个控制器副本分成m个不同的子架构，并且这每个不同的子架构平行训练，训练后的准确度将会被记录，然后计算关于控制器参数θc的梯度，而这个梯度将会被返回到参数服务器，进行迭代更新。 Increase Architecture Complexity with Skip Connections and Other Layer Types(通过跳过连接来增加结构复杂度以及其他网络层类型) We introduce a method that allows our controller to propose skip connections or branching layers, thereby widening the search space. To enable the controller to predict such connections, we use a set-selection type attention which was built upon the attention mechanism. At layer N, we add an anchor point which has N− 1 content-based sigmoids to indicate the previous layers that need to be connected. Each sigmoid is a function of the current hidden state of the controller and the previous hidden states of the previous N − 1 anchor points. We then sample from these sigmoids to decide what previous layers to be used as inputs to the current layer. To be able to add more types of layers, we need to add an additional step in the controller RNN to predict the layer type, then other hyperparameters associated with it. 这篇论文介绍了一种方法，这种方法能够让控制器跳过连接或分支层，进而可以拓宽搜索空间。这种方法引入了注意力机制，在控制器网络第N层增加了一个定位点，这个定位点有N-1个表示前面需要连接的网络层的激活函数。每一个激活函数都是一个关于当前层隐藏状态和前n-1个定位点隐藏状态的函数。这种方法就会根据这个函数来确定前面的网络层是否作为当前层的输入。 为了能够增加更多类型的网络层（不仅仅是卷积层，还有池化层，标准化层等）,需要在控制器的循环神经网络中增加一个额外的步骤来预测网络层的类型和与之关联一些的超参数。 Generate Recurrent Cell Architectures(产生循环网络架构) The computations for basic RNN and LSTM cells can be generalized as a tree of steps that take xt and ht−1 as inputs and produce ht as final output. The controller RNN needs to label each node in the tree with a combination method (addition, elementwise multiplication, etc.) and an activation function (tanh, sigmoid, etc.) to merge two inputs and produce one output. Two outputs are then fed as inputs to the next node in the tree. To allow the controller RNN to select these methods and functions, we index the nodes in the tree in an order so that the controller RNN can visit each node one by one and label the needed hyperparameters. 基本的RNN和LSTM结构的计算，能够被一般化为一棵阶梯树，它以xt和h(t-1)作为输入，产生h(t)作为最后输出。这个控制器循环网络需要标注在阶梯树里的每个节点，并且使用一个结合方法和一个激活函数来合并两个输入并产生一个输出，两个输出之后又被作为树中下一个节点的输入。为了允许控制器循环网络能够选择这些方法和函数，这里为阶梯树上的每一个节点按照次序创建了索引，以便控制器循环网络能够依次遍历每一个节点，并且标注需要的超参数。 Experiments and Results Learning Convolutional Architectures For CIFAR-10(在CIFAR-10数据集上学习卷积神经网络) Training details: The controller RNN is a two-layer LSTM with 35 hidden units on each layer. It is trained with the ADAM optimizer (Kingma &amp; Ba, 2015) with a learning rate of 0.0006. The weights of the controller are initialized uniformly between -0.08 and 0.08. For the distributed train- ing, we set the number of parameter server shards S to 20, the number of controller replicas K to 100 and the number of child replicas m to 8, which means there are 800 networks being trained on 800 GPUs concurrently at any time.Once the controller RNN samples an architecture, a child model is constructed and trained for 50 epochs. The reward used for updating the controller is the maximum validation accuracy of the last 5 epochs cubed. The validation set has 5,000 examples randomly sampled from the training set, the remaining 45,000 examples are used for training. The settings for training the CIFAR-10 child models are the same with those used in Huang et al. (2016a). We use the Momentum Optimizer with a learning rate of 0.1, weight decay of 1e-4, momentum of 0.9 and used Nesterov Momentum (Sutskever et al., 2013).During the training of the controller, we use a schedule of increasing number of layers in the child networks as training progresses. On CIFAR-10, we ask the controller to increase the depth by 2 for the child models every 1,600 samples, starting at 6 layers. Results: 训练细节：控制器循环网络采用两层的LSTM，每层有35个隐藏单元，并采用学习率是0.0006的ADAM优化器。控制器的初始换权重是在-0.08到0.08之间。对于分布式训练来讲，参数服务器分片S是20个，控制器副本K是100个，子网络架构m是8个，所以总共可以有800个网络同时在800个GPU上训练。每一个子网络架构训练50各阶段，更新控制器使用的反馈参数是最后五个阶段中最大的验证准确率。验证集是从训练数据中随机选择的5000个样本，而剩下的45000个样本则作为训练集。在训练控制器的过程中，使用了一个随时间增长的子网络层数，初始层数是6层，每训练1600个样本就增加2层。 训练结果：神经网络架构搜索能够设计几个表现优良的网络架构，其在CIFAR-10上的表现性能和一些最好的模型差不多。 Learning Recurrent Cells for Penn TreeBank(在PTB数据集上学习循环神经网络) Training details: The controller and its training are almost identical to the CIFAR-10 experiments except for a few modifications: 1) the learning rate for the controller RNN is 0.0005, slightly smaller than that of the controller RNN in CIFAR-10, 2) in the distributed training, we set S to 20, K to 400 and m to 1, which means there are 400 networks being trained on 400 CPUs concurrently at any time, 3) during asynchronous training we only do parameter updates to the parameter-server once 10 gradients from replicas have been accumulated. Results: 训练细节：这个实验的控制器循环网络和训练与上面（CIFAR-10）的实验类似，主要有以下几个不同： 优化器的学习率是0.005; 分布式训练中，参数服务器S是20个，控制器副本K是400个，子网络架构m是1个； 在异步训练中，只有控制器副本积累到10个梯度时才更新参数服务器的参数。 训练结果：神经网络搜索获取到的模型在该数据集上表现优于其他一些先进的模型。另外，还可以将获取到的模型通过迁移学习使用在其他的问题解决上。 Conclusion Neural Architecture Search is an idea of using a recurrent neural network to compose neural network architectures. By using recurrent network as the controller, our method is flexible so that it can search variable-length architecture space. Our method has strong empirical performance on very challenging benchmarks and presents a new research direction for automatically finding good neural network architectures. NAS是一种使用循环神经网络生成神经网络架构的想法，通过使用循环神经网络作为控制器，这种方法能够很灵活的用于搜索变长的架构空间，而且在一些数据集上具有很好的实验性能，为自动化查找神经网络架构提供了一个新的研究方向。 Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation Liu, C., Chen, L.-C., Schroff, F., Adam, H., Hua, W., Yuille, A., &amp; Fei-Fei, L. (2019). Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation. Retrieved from http://arxiv.org/abs/1901.02985 Introduction In this paper, we study Neural Archi- tecture Search for semantic image segmentation, an important computer vision task that assigns a label like “person” or “bicycle” to each pixel in the input image. The vast majority of current works on NAS follow this two-level hierarchical design, but only automat- ically search the inner cell level while hand-designing the outer network level. We propose a network level architecture search space that augments and complements the much-studied cell level one, and consider the more challenging joint search of network level and cell level architectures. We develop a differentiable, continuous formulation that conducts the two-level hierarchical architecture search efficiently in 3 GPU days. On PASCAL VOC 2012 and ADE20K, our best model outper- forms several state-of-the-art models. 这篇论文将神经网络架构搜索运用在图像语义分割上。图像语义分割是计算即视觉方面的一个重要任务，目的是给图像中的每一个像素点都打上标签。 当前很多的NAS主要遵循一个两级的分层设计，但是只自动化搜索内部的核单元设计，至于外部的网络设计则采用人工设计。这篇论文提出了一个外部网络架构搜索空间，增加了研究较多的内部核单元搜索，并且进一步考虑了联合外部网络的搜索和内部核单元的搜索。 这篇论文也提出了一个可微分的连续方程，能够使两级分层架构能够在使用GPU的情况下3天完成任务。此外，根据这种方法得到模型在一些数据集上表现性能良好。 Related Work Convolutional neural networks deployed in a fully convolutional manner (FCNs) have achieved remarkable performance on several semantic segmentation benchmarks. Within the state-of-the-art systems, there are two essential components: multi-scale context module and neural network design. In this work, we apply neural architecture search for network backbones specific for semantic segmentation. We further show state-of-the-art performance without ImageNet pretraining, and significantly outperforms FRRN and GridNet on Cityscapes. Several papers used reinforcement learning (either policy gradients or Q-learning ) to train a recurrent neural network that represents a policy to generate a sequence of symbols specifying the CNN architecture. An alternative to RL is to use evolutionary algorithms (EA), that “evolves” architectures by mutating the best architectures found so far. These RL and EA methods tend to require massive computation during the search, usually thousands of GPU days. Our work follows the differentiable NAS formulation and extends it into the more general hierarchical setting. Our work still uses this cell level search space to keep consistent with previous works. Yet one of our contributions is to propose a new, general-purpose network level search space, since we wish to jointly search across this two-level hierarchy. 把卷积神经网络运用到一个全卷积网络里，可以很好的解决语义分割的问题，但是要真正实现这个全卷积网络需要精心的设计和多范围数据的模块。这篇论文里将NAS用于解决这个问题，可以在不需要预训练的情况下获得一个很好的模型。 一些论文中使用强化学习来训练循环神经网络进而产生CNN架构的符号序列，一个强化学习的变体，进化算法也能通过”变异“来产生最终架构。这些方法在查找过程中都需要耗费大量的计算资源，而这篇论文则使用了可微分的NAS公式，并且将它推广到更通用的分层设置中，使得不必要挨个的训练模型，可以节省很多开销。 此外，这篇论文使用了之前工作中的核单位搜索空间，并加入了自己的贡献，即提出了一个新的，外在网络搜索空间，并且尽量通过两级分层结构将二者联合搜索。 Architecture Search Space For the inner cell level , we reuse the one adopted in to keep consistent with previous works. For the outer network level , we propose a novel search space based on observation and summarization of many popular designs. We define a cell to be a small fully convolutional module,typically repeated multiple times to form the entire neural network. More specifically, a cell is a directed acyclic graph consisting of B blocks. We propose the following network level search space. The beginning of the network is a two-layer “stem” structure that each reduces the spatial resolution by a factor of 2. After that, there are a total of L layers with unknown spatial resolutions, with the maximum being downsampled by 4 and the minimum being downsampled by 32. Since each layer may differ in spatial resolution by at most 2, the first layer after the stem could only be either downsampled by 4 or 8. We illustrate our network level search space in Fig. 1. Our goal is then to find a good path in this L-layer trellis. 这篇论文，在内部核单元搜索空间部分，重用了之前工作中的搜索空间，而在外部网络结构部分提出了一个新的基于对一些流行的设计的观察和总结而得出的搜索空间。 外部网络的搜索空间，这个网络的开始是一个两层的茎干部分，逐个减少两倍空间分辨率，在此之后是一个L层的未知空间分辨率，最大的是从4开始降采样，最小的是从32开始降采样。这个网络如图一所示，目标是在这个L层的格子里找到一个合适的路径。 Methods The advantage of introducing this continuous relaxation is that the scalars controlling the connection strength between different hidden states are now part of the differentiable computation graph. Therefore they can be optimized efficiently using gradient descent. We adopt the first-order approximation and partition the training data into two disjoint sets trainA and trainB. We decode the discrete cell architecture by first retaining the 2 strongest predecessors for each block , and then choose the most likely operator by taking the argmax. Quite intuitively, our goal is to find the path with the “max- imum probability” from start to end. This path can be decoded efficiently using the classic Viterbi algorithm, as in our implementation. 这篇论文先通过公式将核架构和网络架构进行连续化操作，这样两层架构均变成了可微分的计算图，进而可以使用梯度下降的方法对其进行优化。针对核架构的解码，通过保留每一块最优的两个前置参数，并通过argmax选择最可能的运算符。针对外部网络架构的编码，目的是要找到一条从开始到结束的最大路径，这个路径可以使用经典的Viterbi算法编码。 Experimental Results We consider a total of L = 12 layers in the network, and B = 5 blocks in a cell. The network level search space has 2.9 × 104 unique paths, and the number of cell structures is 5.6 × 1014. So the size of the joint, hierarchical search space is in the order of 1019. The architecture search optimization is conducted for a total of 40 epochs. The batch size is 2 due to GPU mem- ory constraint. When learning network weights w, we use SGD optimizer with momentum 0.9, cosine learning rate that decays from 0.025 to 0.001, and weight decay 0.0003. The initial values of α, β before softmax are sampled from a standard Gaussian times 0.001. They are optimized using Adam optimizer [36] with learning rate 0.003 and weight decay 0.001. On Cityscapes, Auto-DeepLab significantly outperforms the previous state-of-the-art by 8.6%, and per- forms comparably with ImageNet-pretrained top models when exploiting the coarse annotations. On PASCAL VOC 2012 and ADE20K, Auto-DeepLab also outperforms several ImageNet-pretrained state-of-the-art models. 将NAS使用在语义分割上，经过在不同数据集上的实验，可以得到：在Cityscapes数据集上，auto-deepLab 明显的比其他一些经过ImageNet预训练的先进的方法表现良好，而且和经过ImageNet预训练的顶级模型相比性能差不多；在PASCAL VOC2012和ADE20K的数据集上，auto-deepLab和一部分经过ImageNet预训练的先进模型效果差不多，但表现效果并不是最好的。 Conclusion For future work, within the current framework, related applications such as object detection should be plausible; we could also try untying the cell architecture α across different layers with little computation overhead. Beyond the current framework, a more general network level search space should be beneficial. 这种将核架构搜索和外部网络架构搜索一同使用的NAS是一个较好的发展方向，未来也可以应用于其他相关领域，比如物体检测。此外也可以从减少计算资源的角度着手，这样可以使得NAS有更广泛的应用场景。 Learning Transferable Architecture for Scalable Image Recognition Zoph, B., Vasudevan, V., Shlens, J., &amp; Le, Q. V. (2018). Learning Transferable Architectures for Scalable Image Recognition. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 8697–8710. https://doi.org/10.1109/CVPR.2018.00907 Introduction In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset. We design a search space (which we call “the NASNet search space”) so that the complexity of the architecture is independent of the depth of the network and the size of input images. Our main result is that the best architecture found on CIFAR-10, called NASNet. The image features learned by NASNets are generically useful and transfer to other computer vision problems. 这篇论文提出了一种新的设计卷积网络架构的方法，并且优化了一些基于数据集的网络架构，设计了一个搜索空间，这个搜索空间被称为NASNet搜索空间。这种搜索空间使网络架构的复杂度独立于网络的深度和输入图片的大小。另外，这篇论文的主要成果就是在CIFAR-10数据集上得到的最优架构，即NASNet。从NASNet中提取出的图片特征一般均能转移到其他的计算机视觉问题。 Related Work The proposed method is related to previous work in hyperparameter optimization. The design of our search space took much inspiration from LSTMs, and Neural Architecture Search Cell. 这篇论文和之前的一些超参数优化的工作相关，搜素空间的设计则来源于LSTM和NAS单元。 Method The main contribution of this work is the design of a novel search space, such that the best architecture found on the CIFAR-10 dataset would scale to larger, higher- resolution image datasets across a range of computational settings. In our approach, the overall architectures of the convolutional nets are manually predetermined. They are composed of convolutional cells repeated many times where each convolutional cell has the same architecture, but different weights. To easily build scalable architectures for images of any size, we need two types of convolutional cells to serve two main functions when taking in a feature map as input: (1) convolutional cells that return a feature map of the same dimension, and (2) convolutional cells that return a feature map where the feature map height and width is reduced by a factor of two. We name the first type and second type of convolutional cells Normal Cell and Reduction Cell respectively. The Reduction and Normal Cell could have the same architecture, but we empirically found it beneficial to learn two separate architectures. In our search space, each cell receives as input two initial hidden states hi and hi−1 which are the outputs of two cells in previous two lower layers or the input image. The controller RNN recursively predicts the rest of the structure of the convolutional cell, given these two initial hidden states(Figure 3). 在这篇论文的方法中，全部的卷积网络的架构是人工预先定义的，这些架构是由重复多次的卷积核组成，这些卷积核具有相同的结构和不同的权重。为了能够方便地构建针对任意大小图片的可拓展架构，这里提供了两种类型的卷积核，对于输入的特征图片分别对应不同的功能。一种能返回同样维度的特征图谱，称为Normal核；一种能够让特征图谱的宽高减半，称为Reduction核。这两种卷积核可以有同样的架构，但经过实验发现，不同的架构效果更好。 在搜索空间中，每一个卷积核以两个初始隐藏层状态作为两个输入参数，这两个参数要么是两个卷积核的输出，要么是输入的图像信息。控制器循环网络基于这两个参数递归预测卷积核剩下的结构。 Experiments and Results As can be seen from the Table 1, a large NASNet-A model with cutout data augmentation [12] achieves a state-of-the-art error rate of 2.40% (averaged across 5 runs), which is slightly better than the previous best record of 2.56% by [12]. We find that the learned convolutional cells are flexible across model scales achieving state-of-the-art performance across almost 2 orders of magnitude in computational budget. Increasing the spatial resolution of the input image results in the best reported, single model result for object detection of 43.1%, surpassing the best previous best by over 4.0% . NASNet provides superior, generic image features that may be transferred across other computer vision tasks. The best model identified with RL is significantly better than the best model found by RS by over 1% as measured by on CIFAR-10. 这篇论文在CIFAR-10和ImageNet数据集上进行了图像分类实验，得出的结果比之前的一些先进的方法略优一些。另外还将在ImageNet上预训练好的NASNet网络移入faster-rcnn框架进行了物体检测实验，数据集是COCO数据集，结果表现性能也不错，说明了这个架构可以迁移来解决到其他计算机视觉问题。最后还用实验说明了，使用强化学习（RL）进行架构搜索得到的模型结果比随机搜索（RS）得到的模型结果表现好。 Conclusion The learned architecture is quite flexible as it may be scaled in terms of computational cost and parameters to easily address a variety of problems. The key insight in our approach is to design a search space that decouples the complexity of an architecture from the depth of a network. we demonstrate that we can use the resulting learned architecture to perform ImageNet classification with reduced computational budgets that outperform streamlined architectures targeted to mobile and embedded platforms. 这篇论文提出的NASNet能够很灵活地可以扩展到计算机视觉的其他问题的解决上，而且NASNet搜索空间也将架构的复杂性与网络的深度分离开。 Random Search and Reproducibility for Neural Architecture Search Li, L., &amp; Talwalkar, A. (2019). Random Search and Reproducibility for Neural Architecture Search. 1–20. Retrieved from http://arxiv.org/abs/1902.07638 Introduction We see three fundamental issues with the current state of NAS research: Inadequate Baselines. Complex Methods. Lack of Reproducibility. We help ground existing NAS results by providing a new perspective on the gap between traditional hy- perparameter optimization and leading NAS methods. We identify a small subset of NAS components that are sufficient for achieving good empirical results. We open-source all of the necessary code, random seeds, and documentation necessary to reproduce our experiments. 这篇论文发现了现存的NAS研究里的三种基本的问题，分别是：基准不足，方法复杂，难以复现。 这篇论文的贡献是：在传统的超参数调优方法和先进的NAS方法的比较中提出了新的看法；从NAS组件中选出了一部分足以实现好的实验效果的子集；开源了必要的代码，随机种子和复现实验时所需文档。 Related Work We choose to use a simple method combining random search with early-stopping called ASHA to provide a competitive baseline for standard hyperparameter optimization. Our combination of random search with weight-sharing greatly simplifies the training routine and we identify key variables needed to achieve competitive results on both CIFAR-10 and PTB benchmarks. We follow DARTS and report the result of our random weight-sharing method across multiple trials; in fact, we go one step further and evaluate the broad reproducibility of our results with multiple sets of random seeds. 在相关工作这一部分，这篇论文针对NAS研究里的三个问题分别列举了当前的一些研究工作，并给出了自己相应的解决方案。关于基准不足，这篇论文使用了结合早停的随机搜索的方法，这个方法给标准的超参数调优提供了一个有竞争力的基准；关于方法复杂，这篇论文使用了结合参数共享的随机搜索方法，进而简化了训练过程；关于难以复现，这篇论文给出了复现的相关文档和随机种子，并对结果的可复现性进行了评测。 Methodology Our algorithm is designed for an arbitrary search space with a DAG representation, we use the same search spaces as that considered by DARTS [34] for the standard CIFAR-10 and PTB NAS benchmarks. There are a few key meta-hyperparameters that impact the behavior of our search algorithm: Training epochs.Batch size.Network size.Number of evaluated architectures. Since we train the shared weights using a single architecture at a time, we have the option of only loading the weights associated with the operations and edges that are activated into GPU memory. Hence, the memory footprint of our random search with weight-sharing can be reduced to that of a single model. 这篇论文的随机搜索算法使用的搜索空间和DARTS在CIFAR-10和PTB上使用的搜索空间相同。另外，有一些元超参数能够影响这篇论文的搜索算法，比如训练次数，分块大小，网络大小和评估架构的数量。最后，因为这篇论文在训练共享权重时每次只使用一个架构，所以每次只需要将和架构相关的权重和操作加载进GPU存储里就行，因此这里的结合权重共享的随机搜索算法能够节省GPU开销。 Experiments To evaluate the performance of random search with weight-sharing on these two benchmarks, we proceed in the same three stages: Stage 1: Perform architecture search for a cell block on a cheaper search task. Stage 2: Evaluate the best architecture from the first stage by retraining a larger, network formed from multiple cell blocks of the best found architecture from scratch. This stage is used to select the best architecture from multiple trials. Stage 3: Perform the full evaluation of the best found architecture from the second stage by either training for more epochs (PTB) or training with more seeds (CIFAR-10). For the PTB benchmark, we refer to the network used in the first stage as the proxy network and the network in the later stages as the proxyless network. We next present the final search results. We subsequently explore the impact of various meta-hyperparameters on random search with weight-sharing, and finally evaluate the reproducibility of various methods on this benchmark. Comparison with state-of-the-art NAS methods and manually designednetworks. Lower test perplexity is better on this benchmark. For the CIFAR-10 benchmark,the DAG considered for the convolutional cell has N = 4 search nodes and the operations considered include 3 × 3 and 5 × 5 separable convolutions, 3 × 3 and 5 × 5 dilated separable convolutions, 3 × 3 max pooling, and 3 × 3 average pooling, and zero.Comparison with state-of-the-art NAS methods and manually designed networks. 为了评估在在PTB和CIFAR-10两个基准上权重共享的随机搜索的性能，这篇论文进行了相同的三个阶段: 阶段1:在一个资源耗费较少的搜索任务上对一个单元格块执行架构搜索。 阶段2:通过重新训练一个更大的网络来评估第一个阶段的最佳架构，这个网络由多个单元块组成，这些单元块是从零开始找到的最佳架构。这个阶段用于从多个测试中选择最佳架构。 阶段3:对第二阶段发现的最佳体系结构进行全面评估，方法是进行更多epoch (PTB)的训练，或者进行更多seed (CIFAR-10)的训练。 对在两个基准上分别进行上述三个阶段的操作，最终对比一些已有的NAS方法和人工调参，得到上述两个对比表，可以看出，在PTB基准上能够得到比其他方法较小的perplexity，而在CIFAR-10基准上能够得到相对较少的errors。 Conclusion Better baselines that accurately quantify the performance gains of NAS methods. Ablation studies that isolate the impact of individual NAS components. Reproducible results that engender confidence and foster scientific progress. 这篇论文成果有，提出了一个基本的基准（即论文中随机搜索的方法）来准确地衡量NAS方法的性能收益；对多个NAS组件的影响进行了探索；使NAS实验结果可复现，推动了科研进程。 DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH Liu, H., Simonyan, K., &amp; Yang, Y. (2018). DARTS: Differentiable Architecture Search. 1–13. Retrieved from http://arxiv.org/abs/1806.09055 Introduction The best existing architecture search algorithms are computationally demanding despite their remark- able performance.In this work, we approach the problem from a different angle, and propose a method for efficient architecture search called DARTS (Differentiable ARchiTecture Search).Instead of searching over a discrete set of candidate architectures, we relax the search space to be continuous, so that the architecture can be optimized with respect to its validation set performance by gradient descent. 现有的最佳体系结构搜索算法尽管具有可评注的性能，但在计算上要求很高。这篇论文从不同的角度来解决这个问题，并提出了一种有效的架构搜索方法，称为可微架构搜索DARTS。这篇论文没有在一组离散的候选架构上进行搜索，而是将搜索空间放宽为连续的，这样就可以通过梯度下降对架构的验证集性能进行优化，进而提高计算效率。 Differentiable Architecture Search We search for a computation cell as the building block of the final architecture. The learned cell could either be stacked to form a convolutional network or recursively connected to form a recurrent network. To make the search space continuous, we relax the categorical choice of a particular operation to a softmax over all possible operations. After relaxation, our goal is to jointly learn the architecture α and the weights w within all the mixed operations. 这篇论文用一个计算单元作为最终架构的构建块。这个学习单元可以堆叠成卷积网络，也可以递归连接成递归网络。为了使搜索空间连续，这篇论文将特定操作的分类选择放宽到所有可能操作的softmax，之后联合学习架构α和权重w。 Experiments and Results Our experiments on CIFAR-10 and PTB consist of two stages, architecture search and architecture evaluation. In the first stage, we search for the cell architectures using DARTS, and determine the best cells based on their validation performance. In the second stage, we use these cells to construct larger architectures, which we train from scratch and report their performance on the test set. We also investigate the transferability of the best cells learned on CIFAR-10 and PTB by evaluating them on ImageNet and WikiText-2 (WT2) respectively. DARTS achieved comparable results with the state of the art (Zoph et al., 2018; Real et al., 2018) while using three orders of magnitude less computation resources. Table 2 presents the results for recurrent architectures on PTB, where a cell discovered by DARTS achieved the test perplexity of 55.7. This is on par with the state-of-the-art model enhanced by a mixture of softmaxes (Yang et al., 2018), and better than all the rest of the architectures that are either manually or automatically discovered. 这篇论文在CIFAR-10和PTB上的实验分为架构搜索和架构评估两个阶段。 在第一个阶段，使用DARTS搜索单元结构，并根据它们的验证性能确定最佳单元。 在第二阶段中,使用这些单元来构建更大的架构，从头开始训练并报告他们在测试集上的表现。 另外，这篇论文也调查的最佳单元的可转让性，并评估这个单元在ImageNet和WikiText-2 (WT2)上的性能。 Conclusion We presented DARTS, a simple yet efficient architecture search algorithm for both convolutional and recurrent networks. By searching in a continuous space, DARTS is able to match or outperform the state-of-the-art non-differentiable architecture search methods on image classification and language modeling tasks with remarkable efficiency improvement by several orders of magnitude. 这篇论文提出了一种简单而有效的卷积和递归网络结构搜索算法DARTS。通过在连续空间中搜索，DARTS能够在图像分类和语言建模任务上匹配或优于目前最先进的不可微架构搜索方法，效率显著提高了几个数量级。]]></content>
      <categories>
        <category>Paper Notes</category>
      </categories>
      <tags>
        <tag>AutoML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络结构搜索综述笔记]]></title>
    <url>%2F2019%2F08%2F14%2FNASSurveyNotes%2F</url>
    <content type="text"><![CDATA[NAS_Survey_Notes_Ⅰ Elsken, T., Metzen, J. H., &amp; Hutter, F. (2019). Neural Architecture Search. 20, 63–77. https://doi.org/10.1007/978-3-030-05318-5_3 First Section: Introduction​ NAS (Neural Architecture Search) methods can be categorized to three dimensions: search space, search strategy, and performance estimation strategy. A search strategy selects an architecture A from a predefined search space の，The architecture is passed to a performance estimation strategy, which returns the estimated performance of A to the search strategy. Search Space: The search space defines which architectures can be represented in principle. Search Strategy: The search strategy details how to explore the search space (which is often exponentially large or even unbounded) Performance Estimation Strategy: Performance Estimation refers to the process of estimating this performance: the simplest option is to perform a standard training and validation of the architecture on data, but this is unfortunately computationally expensive and limits the number of architectures that can be explored. Much recent research therefore focuses on developing methods that reduce the cost of these performance estimations. NAS(Neural Architecture Search)神经网络结构搜索方法可以分为三个维度：搜索空间，搜索策略和性能评估策略 搜索策略通过搜索预先定义好的搜索空间，获得候选结构，然后候选结构经过性能评估策略的评估，返回其对应的评估性能，之后反复迭代上述过程，直到选出符合要求性能的网络结构。 搜索空间定义了理论上能够表示的神经网络结构。 搜索策略是指如何探测搜索空间（搜索空间比较大，甚至没有边界） 性能评估策略，则是评估候选网络结构的性能，最简单的方式就是训练和验证这个网络结构，但这样会耗费大量的计算，而且比较耗时，所以需要探索新的方法来减少性能评估的时间和计算开销。 Second Section: Search SpaceThe space of chain-structured neural networks is a relatively simple search space. It is parameterized by (Ⅰ)the (maximum) number of layers n (possibly unbounded); (Ⅱ)the type of operation every layer executes; (Ⅲ)hyperparameters associated with the operation. The space of multi-branch architectures is complex search space. As for this multi-branch architectures , we can search for such motifs, dubbed cells or blocks, respectively, rather than for whole architectures. Zoph et al. (2018) optimize two different kind of cells: a normal cell that preserves the dimensionality of the input and a reduction cell which reduces the spatial dimension. The final architecture is then built by stacking these cells in a predefined manner. This cell-based search space has three major advantages compared with the whole search space: The size of the search space is drastically reduced since cells usually consist of significantly less layers than whole architectures. Architectures built from cells can more easily be transferred or adapted to other data sets by simply varying the number of cells and filters used within a model. Creating architectures by repeating building blocks has proven a useful design principle in general. However, a new design-choice arises when using a cell-based search space, namely how to choose the macro-architecture: how many cells shall be used and how should they be connected to build the actual model?And one direction of optimizing macro-architectures is the hierarchical search space. 链结构神经网络的空间是一个相对简单的搜索空间。它的参数由以下三部分组成：（Ⅰ）（最大）层数n（可能无界）; （二）每层执行的操作类型; （Ⅲ）与操作相关的超参数。 多分支架构的空间是复杂的搜索空间。对于这种多分支架构，我们可以分别搜索单元格或块，而不是整个架构。 Zoph等人。 （2018）优化两种不同类型的单元：保持输入维度的正常单元和减小空间维度的还原单元。然后通过以预定义的方式堆叠这些单元来构建最终的体系结构。与整个搜索空间相比，这种基于单元的搜索空间具有三大优势： 1.搜索空间的大小大大减少，因为单元的层数往往比整个体系结构层数少得多。2.通过简单地改变模型中使用的单元和过滤器的数量，可以更容易地将从单元构建的架构转移或适应其他数据集。3.通过重复构建块来创建体系结构已经被证明是一种有用的设计原则。 然而，当使用基于单元的搜索空间时，出现了一种新的设计选择，即如何选择宏架构：应该使用多少个单元以及如何连接它们来构建实际模型？优化宏架构的一个方向是使用分层搜索空间。 Third Section: Search StrategyMany different search strategies can be used to explore the space of neural architectures, including random search, Bayesian optimization, evolutionary methods, reinforcement learning (RL), and gradient-based methods. Real et al. (2019) conduct a case study comparing RL, evolution, and random search (RS), concluding that RL and evolution perform equally well in terms of final test accuracy, with evolution having better anytime performance and finding smaller models. Both approaches consistently perform better than RS in their experiments, but with a rather small margin: RS achieved test errors of approximately 4% on CIFAR-10, while RL and evolution reached approximately 3.5%. 许多不同的搜索策略可用于探索神经架构的空间，包括随机搜索，贝叶斯优化，进化方法，强化学习（RL）和基于梯度的方法。 Real等人（2019）比较RL，进化算法和随机搜索（RS）这三个算法，得出结论：RL和进化算法在最终测试准确性方面表现同样良好，并且进化算法具有更好的性能，而且找到更小的模型。 两种方法在实验中始终表现优于RS。 Fourth Section: Performance Estimation Speed-up Strategy Speed-up method How are speed-ups achieved? Lower fidelity estimates低保真估计 Training time reduced by training for fewer epochs, on subset of data, downscaled models, downscaled data.通过训练更少的阶段，数据子集，缩减模型，缩减的数据来减少训练时间。 Learning Curve Extrapolation学习曲线外推 Training time reduced as performance can be extrapolated after just a few epochs of training在几个训练阶段完成之后推断根据学习曲线推断模型性能进而减少训练时间 Weight Inheritance / Network Morphisms权重继承或网络态射 Instead of training models from scratch, they are warm-started by inheriting weights并不是从头开始训练模型，而是继承了权重，可以减少训练次数 One-shot Models/Weight Sharingone-shot模型或权重共享 Only the one-shot model needs to be trained; its weights are then shared across different architectures that are just subgraphs of the one-shot model.只需要训练一个one-shot模型，它的权重被不同的结构所使用，这些不同的结构可以看作是one-shot模型的子图 Appendix Ⅰ : This Survey Mind map]]></content>
      <categories>
        <category>Paper Notes</category>
      </categories>
      <tags>
        <tag>AutoML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化机器学习综述笔记]]></title>
    <url>%2F2019%2F08%2F06%2FAutoMLSurveyNotes%2F</url>
    <content type="text"><![CDATA[Auto Machine Learning Survey Notes(Ⅰ) Paper Name: Yao, Q., Wang, M., Chen, Y., Dai, W., Yi-Qi, H., Yu-Feng, L., … Yang, Y. (2018). Taking Human out of Learning Applications: A Survey on Automated Machine Learning. 1–26. Retrieved from http://arxiv.org/abs/1810.13306 First Section In first section,this paper first explains why we need to investigate AutoML. Its reason is following. Machine Learning has been popular recently , but every aspect of machine learning applications, such as feature engineering, model selection, and algorithm selection, needs to be carefully configured, which costs the efforts of many human experts. So we need to research AutoML to take human out of Learning Application and give experts more time to analyze data and problem,and finally promote the development of Machine Learning. This paper also gives some examples of automated machine learning in industry and academic, like Auto-sklearn, Google’s cloud autoML , and Feature Labs, which shows that AutoML has already been successfully applied in many important problems. 在第一部分中，本文首先解释了为什么我们需要研究AutoML。 因为机器学习近些年很流行，但机器学习应用的各个方面，如特征工程，模型选择和算法选择，都需要仔细配置，这需要许多专业人士的努力。所以我们需要研究AutoML以使人类能够从模型训练中解放出来，能够集中精力分析数据和问题本身，最终促进机器学习的发展。 本文还提供了一些工业和学术界自动化机器学习的例子，如Auto-sklearn，Google的 Cloud AutoML，还有Feature Labs，它们表明了AutoML已成功应用于许多重要问题。 Second SectionIn second section, this paper firstly defines AutoML and its core goal. AutoML attempts to construct machine learning programs(specified by Experience, Task and Performance ),without human assitance and within limited computational budgets. And its three core goals are good performance , no assistance from humans and high computational efficiency. Next, this paper propose a basic framework for AutoML approaches. In this framework, an AutoML controller, which has two key ingredients,the optimizer and the evaluator, takes the place of human to find proper configurations for the learning tools. The duty of the evaluator is to measure the performance of the learning tools with configurations provided by the optimizer , while the optimizer’s duty is to update or generate configurations for learning tools. Finally, this paper shows AutoML approaches taxonomies by problem setup and techniques. 在第二部分中，本文首先定义了AutoML及其核心目标。 AutoML尝试构建机器学习程序（由经验，任务和性能指定），无需人工协助并且在有限的计算预算内。 它的三个核心目标是良好的性能，没有人类的帮助和高计算效率。 接下来，本文提出了AutoML方法的基本框架。 在这个框架中，AutoML控制器取代了人类的工作，为学习方法找到合适的配置方式，这个控制器有两个关键组成部分，即优化器和评估器。 在本节的最后，本文将AutoML的方法分为问题设置和技术处理两类。 Third SectionIn third section, this paper gives details on problem setup and introduces some existing AutoML approaches on learning process. AutoML approaches do not necessarily cover the full machine learning pipeline, they can also focus on some parts of the learning process. As for some parts of the learning process, AutoML approaches includes feature engineering , model selection, and optimization algorithm selection. Feature engineering is to automatically construct features from the data so that subsequent learning tools can have good performance. This goal can be divided into two sub-problems. creating features from the data and enhance features’ discriminative ability. However, there are no common or principled methods to create features from data. So , now AutoML focus on feature enhancing methods and there are some common methods to enhance features, dimension reduction,feature generation and feature encoding. Models selection contains two components, picking up some classifiers and setting their corresponding hyper-parameters. The task of model selection is to automatically select classifiers and set their hyper-parameters so that good learning performance can be contained. The goal of optimization algorithm selection is to automatically find an optimization algorithm so that efficiency and performance can be balanced. What’s more, in this section, this paper give two classes of full-scope AutoML approaches. The first one is general case, which is a combination of feature engineering, model selection and algorithm selection. The second one is Network Architecture Search (NAS), which targets at searching good deep network architectures that suit the learning problem. 在第三部分中，本文详细介绍了问题的设置，并介绍了一些现有的AutoML的方法。AutoML可以不必覆盖机器学习的全范围，仅专注于学习过程中的一部分。 针对机器学习的某些部分，AutoML的方法可分为特征工程，模型选择和优化算法选择。特征工程是从数据中自动构造特征，以便后续学习工具可以具有良好的性能。这个目标可以分为两个子问题，一个是从数据创建功能，另一个是增强功能的辨别能力。但是，没有通用或原则方法从数据创建功能。因此，现在AutoML专注于特征增强方法，并且有一些常用方法可以增强特征，例如降维，特征生成和特征编码。模型选择包含两个组成部分，分别是选取分类器和设置其相应的超参数。模型选择的任务是自动选择分类器并设置其超参数，以便可以包含良好的学习性能。优化算法选择的目标是自动找到优化算法，以便平衡效率和性能。 此外，在本节中，本文提供了两类全范围AutoML方法。第一个是一般情况，它是特征工程，模型选择和算法选择的组合。第二个是网络架构搜索（NAS），其目标是搜索适合学习问题的良好深度网络架构。 Fourth SectionIn fourth section, this paper introduces some basic techniques for optimizer. It has three parts including simple search approaches, optimization from samples and gradient descent. Simple search is a naive search approach, and grid search and random search are two common approaches. Optimization from samples is a kind of smarter search approach, and this paper divide existing approaches into three categories, heuristic search, model-based derivative-free optimization, and reinforcement learning. Some popular heuristic search methods are Particle swarm optimization(PSO), Evolutionary Algorithms. The popular methods of model-based derivative-free optimization are Bayesian optimization, classification-based optimization (CBO) and simultaneous optimistic optimization (SOO) . Reinforcement learning (RL) is a very general and strong optimization framework, which can solve problems with delayed feedbacks. Greedy search is a natural strategy to solve multi-step decision-making problem. 在第四部分中，本文介绍了优化器的一些基本技术。 它有三个部分，包括简单搜索方法，样本优化和梯度下降。 简单搜索是一种朴素的搜索方法，其中两种常见的方法分别是网格搜索和随机搜索。样本优化是一种更智能的搜索方法，本文将现有的相关方法分为三类：启发式搜索，基于模型的无导数优化和强化学习。 一些流行的启发式搜索方法是粒子群优化（PSO），进化算法。 基于模型的无导数优化的流行方法是贝叶斯优化，基于分类的优化（CBO）和同时优化优化（SOO）。 强化学习（RL）是一个非常通用且强大的优化框架，可以解决延迟反馈的问题。此外，贪心搜索是解决多步决策问题的常用策略。 Fifth SectionIn fifth section, this paper introduce some basic techniques for evaluator. And the existing methods are direct evaluation, sub-sampling, early stop, parameter reusing and surrogate evaluator. Direct evaluation is often accurate but expensive, and some other methods have been proposed for acceleration by trading evaluation accuracy for efficiency. 在第五部分，本文介绍了评估器的一些基本技术。 现有的方法有直接评估，子抽样，早期停止，参数重用和代理评估。 直接评估通常是准确的但效率不高，而其他的方法是来通过降低评估准确性来提高效率。 Sixth SectionIn sixth section, this paper introduce some experienced techniques , there are two main topics, meta-learning and transfer learning. Meta-learning helps AutoML, on the one hand, by characterizing learning problems and tools.,on the other hand, the meta-learner encodes past experience and acts as a guidance to solve future problems. Existing meta-learning techniques by categorizing them into three general classes based on their applications in AutoML are following: meta-learning for configuration evaluation (for the evaluator). Meta-learners can be trained as surrogate evaluators to predict performances, applicabilities, or ranking of configurations. Representative applications of meta-learning in configuration evaluation are Model evaluation and General configuration evaluation. meta- learning for configuration generation (for the optimizer). The approaches have promising configuration generation, warming-starting configuration generation , and search space refining. meta-learning for dynamic configuration adaptation. Meta-learning can help to automate this procedure by detecting concept drift and dynamically adapting learningdrift. Transfer learning tries to improve the learning on target domain and learning task, by using the knowledge from the source domain and learning task. Transfer learning has been exploited to reuse trained surrogate models or promising search strategies from past AutoML search (source) and improve the efficiency in current AutoML task (target). By transferring knowledge from previous configuration evaluations, we can avoid training model from scratch for the upcoming evaluations and significantly improve the efficiency. 在第六部分，本文介绍了一些比较先进的技术，主要有两个主题，元学习和迁移学习。 元学习辅助AutoML，一方面通过对学习问题和工具的表征。另一方面，通过编码以往的经验来作为对解决未来问题的指导。 基于AutoML中的应用程序，将现有的元学习技术分为三个通用类： 1.用于配置评估的元学习（用于评估器）。元学习器可以作为代理评估器进行训练，以预测性能，适用性或配置排名。元学习在配置评估中的代表性应用有模型评估和一般配置评估这两种。2.用于配置生成的元学习（用于优化器）。相关方法有：良好配置生成，预热配置生成和搜索空间精炼。3.动态配置适应的元学习。元学习可以通过检测概念转换和动态调整学习转换来帮助AutoML。 迁移学习通过使用来自源域和学习任务的知识，尝试改进对目标域和学习任务的学习。 迁移学习能重用过去AutoML任务（源）经过训练的代理模型或优良的搜索策略，并提高当前AutoML任务（目标）的效率。通过从以前的配置评估中迁移知识，可以避免从头训练模型，显著提高效率。 Seventh SectionIn seventh section, this paper introduce three AutoML examples, Auto-sklearn, NASNet and ExploreKit. In Auto-sklearn, model selection is formulated as a CASH problem, which aims to minimize the validation loss with respect to the model as well as its hyper-parameters and parameters. RL is employed in NAS to search for a optimal sequence of design decisions. Besides, the direct evaluation is used in these works as the evaluator. As RL is slow to converge, to make the search faster, transfer learning, which is firstly used to cut the search space into several blocks. ExploreKit conducts more expensive and accurate evaluations on the candidate features. Error reduction on the validation set is used as the metric for feature importance. Candidate features are evaluated successively according to their ranking, and selected if their usefulness surpass a threshold. This procedure terminates if enough improvement is achieved. The selected features will be used to generate new candidates in the following iterations. 在第七部分中，本文介绍了三个AutoML应用示例，Auto-sklearn，NASNet和ExploreKit。 在Auto-sklearn中，模型选择被公式化为CASH问题，其目的是最小化模型的验证损失函数及其超参数和参数。 NAS使用RL来搜索最佳的设计决策序列，使用直接评估作为整个工作的评估器。但RL收敛缓慢，为了使搜索更快，NAS使用了转移学习，在搜索之前将搜索空间切割成几个块，以加快收敛速度。 ExploreKit对候选功能进行更准确但更低效的评估。将验证集上的错误减少情况用作特征有效性的度量标准，按照相应排名对候选特征进行评估，并选择有效性超过阈值的特征，所选的特征将在以下的迭代中生成新候选。如果实现了足够的效果，则迭代过程终止。 Eighth SectionIn eighth section, this paper reviews the history of AutoML, summarizes how its current status in the academy and industry, and discuss its future work. AutoML only becomes practical and a big focus recently, due to the big data, the increasing computation of modern computers, and of course, the great demand of machine learning application. AutoML is a very complex problem and also an extremely active research area, and there are many new opportunities and problems in AutoML. And there are also many workshops and competitions. Higher efficiency can be achieved by either proposing algorithms for the optimizer, which visit less configurations before reaching a good performance, or designing better methods for the evaluator, which can offer more accurate evaluations but in less time. 在第八部分，本文回顾了AutoML的历史，总结了其在学术界和行业中的现状，并讨论了其未来的发展方向。 由于大数据，现代计算机的计算量不断增加，当然还有机器学习应用的巨大需求，AutoML最近才成为重点。 AutoML是一个非常复杂的问题，也是一个非常活跃的研究领域，AutoML中存在许多新的机会和问题，还有许多研讨会和比赛。 为优化器提出算法可以实现更高的效率，这样使得优化器在达到良好性能之前访问较少的配置；或者为评估器设计更好的方法，这可以在更短的时间内提供更准确的评估。 Appendix Ⅰ : This Survey Mindmap Auto Machine Learning Survey Notes(Ⅱ) Elshawi, R., Maher, M., &amp; Sakr, S. (2019). Automated Machine Learning: State-of-The-Art and Open Challenges. Retrieved from http://arxiv.org/abs/1906.02287 First SectionIn first section, this paper briefly introduces the development of AutoML and explain what is the CASH (Combined Algorithm Selection and Hyper-parameter tuning) problem. 在第一部分中，本文简要介绍了AutoML的发展，并解释了什么是CASH问题。 Second SectionIn Second section, this paper introduces the various techniques that have been introduced to tackle the challenge of warm starting(meta-learning) for AutoML search problem in the context of machine learning and deep learning domains.. These techniques can generally be categorized into three broad groups: learning based on task properties, learning from previous model evaluations and learning from already pretrained models. 在第二部分中，本文介绍了在机器学习和深度学习领域中针对AutoML搜索问题的热启动（元学习）挑战所引入的各种技术。 这些技术通常可以分为三大类：从任务属性中学习，从先前模型评估中学习以及从已经预训练的模型中学习。 Third SectionIn third section, this paper introduces the various approaches that have been introduced for tackling the challenge of neural architecture search(NAS) in the context of deep learning. Broadly, NAS techniques falls into five main categories including random search, reinforcement learning, gradient- based methods, evolutionary methods, and Bayesian optimization. 在第三部分中，本文介绍了在深度学习环境中应对神经结构搜索（NAS）挑战的各种方法。 从广义上讲，NAS技术分为五大类，包括随机搜索，强化学习，基于梯度的方法，进化方法和贝叶斯优化。 Fourth SectionIn fourth section, this paper introduces some different approaches for automated hyper-parameter optimization. In principle, the automated hyper-parameter tuning techniques can be classified into two main categories: black-box optimization techniques and multi-fidelity optimization techniques. 在第四部分中，本文介绍了一些不同的自动超参数优化的方法。 原则上，自动超参数调整技术可以分为两大类：黑盒优化技术和多保真优化技术。 Fifth SectionIn fifth section, this paper covers the various tools and frameworks that have been implemented to tackle the CASH problem. In general, these tools and frameworks can be classified into three main categories: centralized, distributed, and cloud-based. And Neural Network Automation Frameworks is also a class recently. 在第五部分中，本文介绍了为解决CASH问题而实施的各种工具和框架。 这些工具和框架可以分为三大类：集中式框架，分布式框架和基于云平台的框架。此外，神经网络自动化框架也逐渐发展起来。 Sixth SectionIn sixth section, this paper introduces some state-of-the-art research efforts on tackling the automation aspects for the other building blocks (Pre-modeling and Post-Modeling) of the complex machine learning pipeline. 在第六部分中，本文介绍了一些关于自动化复杂机器学习问题中的其他构建模块（比如预建模和后建模）的前沿研究工作，。 Seventh SectionIn seventh section, this paper introduces some research directions and challenges that need to be addressed in order to achieve the vision and goals of the AutoML process. It includes scalability, optimization techniques, time budget, composability, user friendliness, continuous delivery pipeline, data validation, data preparation, and model deployment and life cycle. 在第七部分中，本文介绍了为了实现AutoML过程的这个目标，需要进一步研究的的一些方向和挑战。它包括可扩展性，优化技术，时间预算，可组合性，用户友好性，持续交付管道，数据验证，数据准备，模型部署和生命周期。 Appendix Ⅰ : This Survey Mind map Auto Machine Learning Survey Notes(Ⅲ) Zöller, M.-A., &amp; Huber, M. F. (2019). Survey on Automated Machine Learning. Retrieved from http://arxiv.org/abs/1904.12054 First SectionIn first section, this paper introduces the history of AutoML, and shows AutoML is no new trend. It also gives this paper’s contributions: introduce a mathematical formulation covering the complete procedure of automatic ML pipeline creation, cover AutoML techniques for each step of building an ML pipeline, and evaluate all presented algorithms. Finally, it shows this paper’s structure. 在第一部分中，本文介绍了自动化机器学习的发展历史，并指出它并不是一个新的趋势。本文还给出了这篇论文的贡献：介绍了一个覆盖整个自动化机器学习过程的数学公式，介绍了构建机器学习流水线的每一步的automl技术，并对所提出的所有算法进行了评估。最后给出了这篇论文的脉络结构。 Second SectionIn second section, this paper gives a mathematical sound formulation of the automatic construction of ML pipelines is given. And it shows most current state-of-the-art algorithms solve the pipeline creation problem in two distinct stages, pipeline structure search, and Algorithm selection and hyperparameters optimization. 第二部分给出了ML流水线自动构造的数学合理公式。并说明了目前最先进的算法如何解决了流水线创建问题，分为两个子问题，分别是流水线结构搜索和算法选择及超参数优化。 Third SectionIn third section, this paper introduces some approaches about pipeline structure creation. a basic ML pipeline layout: At first, the input data is cleaned in multiple distinct steps, like imputation of missing data and one-hot encoding of categorical input. Next, relevant features are selected and new features created in a feature engineering stage. This stage highly depends on the underlying domain. Finally, a single model is trained on the previously selected features. a fixed pipeline shape： Fixed ML pipeline used by most AutoML frameworks. Minor differences exist regarding the implemented data cleaning steps. Regarding data cleaning, the pipeline shape differs. Yet, often the two steps imputation and scaling are implemented. Even though this approach greatly reduces the complexity of the pipeline creation problem, it leads to inferior pipeline performances for complex data sets. Fixed shaped ML pipelines lack this flexibility to adapt to a specific task. some variable shapes: genetic programming, Genetic programming is an iterative, domain-agnostic optimization method derived from biological evolution. hierarchical task networks(HTNs)，HTNs are a method from automated planning that recursively partition a complex problem into easier subproblems. self-play, Self-play (Lake et al., 2017) is a reinforcement learning strategy, the algorithm creates new training examples by playing against itself. A common drawback of self-play approaches is the low convergence speed making this approach rather unsuited for AutoML. 第三部分介绍了一些关于机器学习流水线创建的方法。 首先介绍了一个基本的机器学习流水线布局，在这个布局里，输入数据先被清洗，然后从中提取出特征，最后，根据提取出的特征训练模型。 之后，介绍了固定的流水线布局，这个布局被大部分自动化机器学习框架采用。这个布局与基本的机器学习流水线布局相比，只有在数据清洗的步骤有所差异。在该布局中，数据清洗是由两个步骤完成的，插补和缩放。尽管这种布局极大地减少了流水线创建的复杂度，但它也使得流水线在处理复杂的数据集时性能不高。而且，这种布局还缺乏适应特定任务的灵活性。 最后，介绍了一些流水线布局的变体。第一个是基于遗传编码的变体，这种方法是一中从生物进化借鉴而来的迭代优化方法；第二个是基于层次任务网络的方法，这种方法是一种自动地将复杂问题递归地划分为简单问题的方法。第三个是self-play方法，这个是一种强化学习的策略，这个方法通过与自身对抗创建新的训练示例，但是这个方法的一个常见缺点就是收敛速度低，因此它不是很适合autoML。 Fourth SectionIn fourth section, this paper shows some approaches to solve CASH problem (the combined algorithm selection and hyperparameter optimization, both steps are executed simultaneously) . Grid Search, The first approach proposed to systematically explore the configuration space was grid search. grid search creates a grid of configurations and evaluates all of them. Therefore, each continuous hyperparameter is discretized into k (logarithmic) equidistant values. This basic algorithm does not scale well for large configuration spaces, as the number of function evaluations grows exponentially with the number of hyperparameters. Random Search, A candidate configuration is generated by randomly choosing a value for each hyperparameter independently of all others. Random search is straightforward to implement and parallelize and well suited for gradient-free functions with many local minima . Sequential Model-Based Optimization: Gaussian Process, Even though a Gaussian process is a non-parametric model, it still requires hyperparameters itself, namely the selection of m and k. A common drawback of Gaussian processes is the runtime complexity of O(n3) due to the inversion of K. Random Forests, Random forest regression is an ensemble method consisting of multiple regression trees Tree-structured parzen estimator, TPE natively handles hierarchical search spaces by modeling each hyperparameter individually by two one-dimensional distributions. Evolutionary Algorithm: Genetic Programming, genetic programming should only be used for CASH in combination with ML pipeline structure search. Because some concepts cannot be easily applied to algorithm selection: a crossover between a SVM and a decision tree cannot be modeled reasonably. Particle Swarm Optimization, PSO tends to converge faster to a local minimum in comparison to genetic programming . Multi-Armed Bandit Learning, Multi-armed bandit learning is limited to a finite number of bandits. Gradient Descent, A very powerful optimization method is gradient descent, an iterative minimization algorithm. 在第四部分，论文主要介绍了一些解决CASH问题的方法。 算法选择和超参数优化两个步骤同步进行，这个简称CASH问题。 网格搜索：这是第一个系统搜索配置空间的方法。网格搜索为配置创建了一个网格，并且对它们进行了评估。同时每一个连续的超参数都被等价地离散化为k个值。这个基础的算法不能很好地解决较大配置空间的问题，因为评估函数的数量将会随着超参数的数量指数级增长。 随机搜索：通过随机地为每一个独立的超参数选择一个值，便可以产生一个候选的配置，随机搜索很容易实现，也很容易并行化，非常适合具有许多局部最小值的无梯度函数。 基于序列模型的优化： 高斯过程：尽管一个高斯过程是一个无参数的模型，但它本身依旧需要超参数，比如m和k的选择。高斯过程一个普遍的缺点就是由于k的反转而导致的高时间复杂度O(n3)。 随机森林：随机森林回归是有多个回归树组成一个集成方法 树形结构的parzen估计：TPE通过两个一维分布分别对每个超参数进行建模，进而在本地处理分层搜索空间 进化算法： 遗传编码：遗传编码只能用于CASH和机器学习流水线结构查找相结合，因为某些概念不能轻易应用于算法选择：SVM和决策树之间的交叉无法合理地建模。 粒子群优化算法：和遗传编码相比，PSO更快地收敛到局部最小值。 多臂赌博机学习：仅限于有限数量的赌博机。 梯度下降法：基于迭代的最小化算法。 Fifth SectionData cleaning is an important aspect of building an ML pipeline. The purpose of data cleaning is improving the quality of a data set by removing data errors. Common error classes are missing values in the input data, invalid values or broken links between entries of multiple data sets. 数据清理是构建ML流水线的一个重要方面。 数据清洗的目的通过消除数据错误来提高数据集的质量。 常见错误类别有输入数据中缺少的值，无效值或多个数据集的条目之间的连接缺失。 Sixth SectionThe task of feature engineering is highly domain specific and very difficult to generalize. Even for a data scientist assessing the impact of a feature is difficult, as domain knowledge is necessary. Consequently, feature engineering is a mainly manual and time-consuming task driven by trial and error. Feature selection is the easier part of feature engineering as the search space is very limited: each feature can either be included or excluded. Consequently, many different algorithms for feature selection exist. Basically all automatic feature generation approaches follow the iterative scheme. Based on an initial data set, a set of candidate features is generated and ranked. High ranking features are evaluated and potentially added to the data set. These three steps are repeated several times. 特征工程的任务是和领域高度相关的，很难统一概括。 因此即使是数据科学家也很难对特征的优劣进行评估。 因此，特征工程是一个主要由人类参与且耗时巨大的试错过程。 特征选择是特征工程中比较容易的部分，因为搜索空间非常有限：可以包含或排除某个特征。 现在存在许多用于特征选择的不同算法。 而至于特征生成方面，基本上所有自动特征生成方法都遵循迭代方案。 基于初始数据集，生成并排列一组候选特征。 评估并对这些其中排序靠前的特征，并有可能添加到数据集中。 这三个步骤重复几次，直到生成符合要求的特征。 Seventh SectionIn seventh section, this paper introduces different performance improvements. Multi-Fidelity Approximations: Depending on the used data set, fitting a single model can take several hours, in extreme cases even up to several days. Consequently, optimization progress is very slow. A common approach to circumvent this limitation is the usage of multi-fidelity approximations. By testing a configuration on this training subset, bad performing configurations can be discarded very fast and only well performing configurations have to be tested on the complete training set. Early Stopping: A quite simple approximation is aborting the fitting after the first fold if the performance is significantly worse than the current incumbent. Scalability: A common strategy for solving a computational heavy problem is parallelization on multiple cores or within a cluster. As AutoML normally has to fit many ML models, distributing different fitting instances in a cluster is an obvious idea. Ensemble Learning: Ensemble methods combine multiple ML models to create predictions. Depending on the diversity of the combined models, the overall accuracy of the predictions can be significantly increased. The cost of evaluating multiple ML models is often neglectable considering the performance improvements. Meta-Learning: Meta-learning can be used in multiple stages of automatically building an ML pipeline to increase the efficiency. Search Space Refinements, Filtering of Candidate Configurations, Warm-Starting, Pipeline Structure. 在第七部分，论文介绍了几种不同的优化方法，多保真估计，早停，扩展，集成学习和元学习。 多保真估计：通过测试该训练子集上的配置，可以非常快速地丢弃性能差的配置，并且只需要在完整的训练集上测试性能良好的配置。 早停： 一个非常简单的近似是在第一次折叠后如果性能明显比当前的差，那么中止拟合。 扩展：由于AutoML通常必须适合许多ML模型，因此在一个群集中分配不同的拟合实例是一个直接的想法。 集成学习：集成学习结合多个ML模型来进行预测。 元学习：元学习可以在自动构建ML流水线的多个阶段中使用，以提高效率。 Eighth SectionIn this section, papers introduces some existing frameworks: 在第八部分，论文介绍了一些现有的框架。 Appendix Ⅰ：This Survey Mind map]]></content>
      <categories>
        <category>Paper Notes</category>
      </categories>
      <tags>
        <tag>AutoML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习读书笔记（Ⅰ）]]></title>
    <url>%2F2019%2F07%2F24%2FMachineLearningBookNotes%2F</url>
    <content type="text"><![CDATA[Notes on Machine Learning重要名词解释Important Expressions训练集train set： 用于模型训练的数据集 验证集validation set：用于进行模型评估和模型选择的数据集 测试集test set：用来评估模型在实际使用中的泛化能力 形象上来说训练集就像是学生的课本，学生 根据课本里的内容来掌握知识，验证集就像是作业，通过作业可以知道 不同学生学习情况、进步的速度快慢，而最终的测试集就像是考试，考的题是平常都没有见过，考察学生举一反三的能力。 错误率error rate:分类错误的样本数占样本总数的比例 精度accuracy：分类正确的样本数占样本总数的比例 线性模型Linear Model线性判别分析Linear Discriminant Analysis,LDA:用于解决二分类问题的经典线性学习方法。主要思想：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离。 多分类学习的基本思路是“拆解法”，即将多分类任务拆为若干个二分类任务求解。 最经典的拆分策略有三种：“一对一”（One vs One, OvO)，“一对其余”（One vs Rest，OvR) 和 “多对多”（Many vs Many，MvM） 类别不平衡问题class-imbalance:指分类任务中不同类别的训练样例数目差别很大的情况。三类处理方案，第一类是欠采样（under sampling)即去除训练集中数目较多类别的样例；第二类是过采样（over sampling)即增加训练集中数目较少类别的样例；第三类是直接基于原始训练集进行训练，但在使用分类器进行预测时修改决策过程，进行“阈值移动”（threshold moving)。 决策树Decision Tree决策树的划分目标：决策树的分支节点所包含的样本尽可能属于同一类别，即结点的“纯度”（purity）越来越高。 神经网络Neural Networks多层前馈神经网络（multi-layer feedforward neural networks)：每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络称之为多层前馈神经网络。 神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”（connection weight）以及每个功能神经元的阈值；换言之。神经网络学到的东西，蕴含在连接权与阈值中。 常见的神经网络： RBF(Radial Basis Function，径向基函数)网络，这是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。 ART（Adaptive Resonance Theory， 自适应谐振理论）网络，它采用竞争型学习（一种常用的无监督学习策略）的策略，由比较层、识别层、识别阈值和重置模块构成。 SOM（Self-Organizing Map，自组织映射）网络，这是一种竞争学习型的无监督神经网络，它能将高位输入数据映射到低维空间（通常是二维）,同时保持输入数据在高维空间的拓扑结构，即将高维空间中相似的样本点映射到网络输出层中的邻近神经元。 级联相关网络（cascade-correlation) 是结构自适应的神经网络。 Elman网络是递归神经网络。与前馈神经网络不同，递归神经网络（recurrent neural networks)允许网络中出现环形结构，从而可以让一些神经元的输出反馈回来作为输入信号。 Boltzmann机：这是一种“基于能量的模型”（energy-based model)。神经网络中有一类模型是为网络状态定义一个“能量”（energy），能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。 深度学习deep learning：典型的深度学习模型就是很深层的神经网络。对于神经网络模型，提高容量的一个简单办法就是增加隐层数目，然而多隐层神经网络难以直接使用经典算法（比如标准BP算法）进行训练，因为误差在多隐层内逆传播时，往往会“发散”（diverge）而不能收敛到稳定状态，所以无监督逐层训练（unsupervised layer-wise training)是多隐层网络训练的有效手段，即采用预训练+微调的做法对多隐层网络进行训练，这样可以有效地节省了训练开销。 另一种节省训练开销的策略是”权共享（weight sharing)”，即让一组神经元使用相同的连接权，这个策略在卷积神经网络(Convolutional Neural Network)发挥了重要作用。 贝叶斯分类Bayes Classifier*朴素贝叶斯分类器（naive Bayes classifier) *：朴素贝叶斯分类器采用了属性条件独立假设（attribute conditional independence assumption)，即对已知类别，假设所有属性相互独立，换言之，假设每个属性独立地对分类结果发生影响。之所以使用朴素贝叶斯分类器，是因为贝叶斯分类器难以从有限的训练样本中直接估计所有属性上的联合概率。 EM(Expectation-Maximization)算法：是一种迭代式的方法，用于计算参数隐变量，其基本思想是：若参数Θ已知，则可以根据训练数据推断出最优隐变量Z的值（E步）；反之，若Z的值已知，则可以方便地对参数Θ做最大似然估计（M步）。 集成学习Ensemble Learning*集成学习(ensemble learning) *：通过构建并结合多个学习器来完成学习任务，也被称为多分类器系统（multi-classifier system)。根据个体学习器的生成方式，目前的集成学习方法大致可分为两类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，代表算法是Boosting；另一类是个体学习器之间不存在强依赖关系，可同时生成的并行化方法，代表是Bagging和随机森林（Random Forest)。 Boosting：这是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。 Bagging：这是一种并行式集成学习方法，它直接基于自助采样法（Bootstrap sampling)。给定包含m个样本的数据集，先随机取出一个样本放入采样集中，再把样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到m个样本的采样集。其中初始训练集中约有63.2%的样本出现在采样集中。照这样可以采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合，这就是Bagging的基本流程。 随机森林(Random Forest，RF)：RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统的决策树在选择划分属性时实在当前结点的属性集合（假设有d个属性）中选择一个最优属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最有属性用于划分，这里的参数k控制了随机性的引入程度。 结合策略：平均法，投票法和学习法（通过另一个学习器来结合个体学习器，即通过次级学习器来结合初级学习器）。 聚类Clustering聚类(clustering)：聚类算法是无监督学习（unsupervised learning)，训练样本的标记信息未知，通过聚类，试图将数据集中的样本划分为若干个通常不相交的子集，每个子集称为一个“簇”（cluster)。聚类过程仅能自动形成簇结构，而簇所对应的概念语义需要使用者来进行命名和把握。聚类算法涉及的两个基本问题是性能度量和距离计算。 性能度量(validity index)：性能度量大致有两类： 一类是将聚类结果与某个参考模型（reference model)比较，称为“外部指标”（external index），常用的外部指标有Jaccard系数（Jaccard Coefficient, JC），FM指数（Fowlkes and Mallows Index, FMI)和Rand指数（Rand Index,RI)。这些指数越大说明聚类效果越好。 另一类是直接考察聚类结果而不利用任何参考模型，称为“内部指标”（Internal index)，常用的内部指标有DB指数（Davies-Bouldin Index, DBI）和Dunn指数（Dunn Index,DI)。内部指标中DBI的值越小越好，而DI的值越大越好。 密度聚类（density-based clustering）：通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果。DBSCAN是一种著名的密度聚类算法。 层次聚类（hierarchical clustering）：这种聚类算法试图在不同层次对数据集进行划分，从而形成树形的聚类结构。AGNES是一种采用自底向上聚合策略的层次聚类算法。 半监督学习Semi-supervised Learning半监督学习（semi-supervised learning)：让学习器不依赖外界交互，自动利用未标记样本来提升学习性能。 半监督支持向量机（semi-supervised support vector machine, S3VM)：这是支持向量机在半监督学习上的推广。 半监督聚类（semi-supervised clustering)：聚类是典型的无监督学习任务，但现实的聚类任务中，我们往往能获取到一些额外的监督信息，这些信息大致有两类，一类是必连（指样本必属于同一个簇）与勿连（指样本必不属于同一个簇）约束，另一类是少量的标记样本。利用这些监督信息，我们可以通过半监督聚类来获得更好的聚类效果。 规则学习Rule Learning规则学习（rule learning)：规则学习是从训练数据中学习出一组能用于对未见示例进行判别的规则。 序贯覆盖（sequential covering)：即逐条归纳，在训练集上每学到一条规则，就将该规则覆盖的训练样例去除，然后以剩下的训练样例组成训练集重复上述过程。由于每次只处理一部分数据，因此也被称为“分治”（separate-and-conquer)策略。 一阶规则学习，FOIL算法（First-Order Inductive Learner)：FOIL是著名的一阶规则学习算法，它遵循序贯覆盖框架并且采用自顶向下的规则归纳策略，由于逻辑变量的存在，FOIL在规则生成时需要考虑不同的变量组合。 归纳逻辑程序设计（Inductive Logic Programming，ILP）：归纳逻辑程序设计在一阶规则学习中引入了函数和逻辑表达式嵌套。 强化学习Reinforcement Learning强化学习：强化学习的学习目的就是要找到能使长期累积奖赏最大化的策略。强化学习中没有标记样本，即没有人直接告诉机器在什么状态下应该做什么动作，只有等到最终结果揭晓，才能通过“反思”之前动作是否正确来进行学习，因此，强化学习在某种意义上可看作具有“延迟标记信息”的监督学习问题。 探索与利用(Exploration and Exploitation)：探索（“估计摇臂的优劣”）和利用（“选择当前最优摇臂”）这两者是矛盾的，因为尝试次数有限，加强了一方则会自然削弱另一方，这是强化学习所面临的“探索利用窘境”（Exploration-Exploitation dilemma)。显然，想要强化学习累积奖赏最大，需要在探索和利用之间达到较好的折中。 模仿学习（imitation learning)：在强化学习的经典任务设置中，机器所能获得的反馈信息仅有多步决策后的累计奖赏，但在现实任务中，往往能得到人类专家的决策过程范例，从这样的范例中学习，称为“模仿学习”（imitation learning）。]]></content>
      <categories>
        <category>Book Notes</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习读书笔记（Ⅰ）]]></title>
    <url>%2F2019%2F07%2F24%2FDeepLearningBookNotes%2F</url>
    <content type="text"><![CDATA[Deep Learning Book Notes深度前馈网络deep feedforward network基于梯度的学习——代价函数： 使用最大似然学习条件分布。大多数现代的神经网络使用最大似然来训练。这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价。 均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不佳，一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度，这就是为什么交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一。 基于梯度的学习——输出单元： 代价函数的选择与输出单元的选择密切相关。任何可用作输出的神经网络单元，也可以被用作隐藏单元。 用于高斯输出分布的线性单元：一种基于仿射变换的输出单元，仿射变换不具有非线性，所以这些单元往往直接被称为线性单元。 用于Bernoulli输出分布（伯努利分布）的sigmoid单元：许多任务需要预测而执行变量的值。具有两个类的分类问题可以归结为这种形式。 用于Multinoulli输出分布（多项分布）的softmax单元：任何时候当我们想要表示一个具有n个可能取值的离散型随机变量的分布时，我们都可以使用softmax函数。它可以看作是sigmoid函数的扩展，其中sigmoid函数用来表示二值型变量的分布。 隐藏单元： 整流线性单元及其扩展：整流线性单元易于优化，它与线性单元非常类似。线性单元和整流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为零。 logistic sigmoid与双曲正切函数： 其他隐藏单元：softmax单元，径向基函数（rational basis function，RBF），softplus函数，硬双曲正切函数（hard tanh) 深度学习中的正则化和优化深度学习中的正则化：对学习算法进行修改，旨在减少泛化误差而不是训练误差。模型族训练有三个情形： 不包括真实的数据生成过程，对应欠拟合和含有偏差的情况； 匹配真实数据生成过程； 除了包括真实的数据生成过程，还包括许多其他可能的生成过程——方差（而不是偏差）主导的过拟合。 正则化的目标是使模型从第三种情况转化为第二种情况。 深度学习中的优化：用于深度模型训练的优化算法与传统的优化算法在几个方面有所不同。机器学习通常是间接作用的。在大多数机器学习问题中，我们关注某些性能度量P，其定义于测试集上并且可能是不可解的。因此，我们只是间接地优化P。我们希望通过降低代价函数J(θ)来提高P，这一点与纯优化不同，纯优化最小化目标J本身。训练深度模型的优化算法通常也会包括一些针对机器学习目标函数的特定结构进行优化。 卷积神经网络convolutional neural network卷积神经网络，也叫做卷积网络，指那些至少在网络的一层中使用卷积运算（卷积运算是一种特殊的线性运算）来替代一般矩阵乘法运算的神经网络。这种网络是一种专门用来处理具有类似网格结构的数据的神经网络，比如时间序列数据和图像数据。 卷积运算： s(t) = ∫x(a)w(t - a)da 上面这种运算称为卷积（convolution）,卷积运算通常用星号*表示。 s(t) = (x * w)(t) 在卷积运算的术语中，卷积的第一个参数（上式中的x）通常叫做输入（input)，第二个参数（上式中的函数w）叫做核函数（kernel），输出有时被称为特征映射（feature map)。 动机：卷积运算通过三个重要思想来帮助改进机器学习系统：稀疏交互（sparse interactions)、参数共享（parameter sharing）、等变表示（equivariant representations)。 池化：卷积网络中一个典型层包含三级。第一级中，这一层并行地计算多个卷积产生一组线性激活响应。第二级中，每一个线性激活响应将会通过一个非线性的激活函数，例如整流线性激活函数。这一级有时也被称为探测级。第三级中，我i们使用池化函数来进一步调整这一层的输出。 池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。 循环神经网络recurrent neural network循环神经网络rnn：这是一类用于处理序列数据的神经网络。 循环神经网络中的一些重要的设计模式： 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络。 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络。 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络。 长短期记忆LSTM：引入自循环的巧妙构思，以产生梯度长时间持续流动的路径。]]></content>
      <categories>
        <category>Book Notes</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
</search>
